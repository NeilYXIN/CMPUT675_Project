{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c65609f-63a2-4a64-bba2-b27a2e601e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# Python version: 3.6\n",
    "\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import argparse\n",
    "from options import args_parser\n",
    "from update import LocalUpdate, test_inference\n",
    "from models import MLP, CNNMnist, CNNFashion_Mnist, CNNCifar\n",
    "from utils import get_dataset, average_weights, exp_details\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a204d617-f6ab-4248-b5f2-daad5da4ddf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "start_time = time.time()\n",
    "\n",
    "# define paths\n",
    "path_project = os.path.abspath('..')\n",
    "logger = SummaryWriter('../logs')\n",
    "\n",
    "args = args_parser()\n",
    "exp_details(args)\n",
    "\n",
    "device = 'cuda' if args.gpu else 'cpu'\n",
    "\n",
    "if args.gpu:\n",
    "    torch.cuda.set_device(args.gpu)\n",
    "\n",
    "# load dataset and user groups\n",
    "train_dataset, test_dataset, user_groups = get_dataset(args)\n",
    "\n",
    "# BUILD MODEL\n",
    "if args.model == 'cnn':\n",
    "    # Convolutional neural netork\n",
    "    if args.dataset == 'mnist':\n",
    "        global_model = CNNMnist(args=args)\n",
    "    elif args.dataset == 'fmnist':\n",
    "        global_model = CNNFashion_Mnist(args=args)\n",
    "    elif args.dataset == 'cifar':\n",
    "        global_model = CNNCifar(args=args)\n",
    "\n",
    "elif args.model == 'mlp':\n",
    "    # Multi-layer preceptron\n",
    "    img_size = train_dataset[0][0].shape\n",
    "    len_in = 1\n",
    "    for x in img_size:\n",
    "        len_in *= x\n",
    "        global_model = MLP(dim_in=len_in, dim_hidden=64,\n",
    "                           dim_out=args.num_classes)\n",
    "else:\n",
    "    exit('Error: unrecognized model')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b34fd6-cd2c-4499-a6e7-28972aef7af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to train and send it to device.\n",
    "global_model.to(device)\n",
    "global_model.train()\n",
    "print(global_model)\n",
    "\n",
    "# copy weights\n",
    "global_weights = global_model.state_dict()\n",
    "\n",
    "# Training\n",
    "train_loss, train_accuracy = [], []\n",
    "val_acc_list, net_list = [], []\n",
    "cv_loss, cv_acc = [], []\n",
    "print_every = 2\n",
    "val_loss_pre, counter = 0, 0\n",
    "\n",
    "for epoch in tqdm(range(args.epochs)):\n",
    "    local_weights, local_losses = [], []\n",
    "    print(f'\\n | Global Training Round : {epoch+1} |\\n')\n",
    "\n",
    "    global_model.train()\n",
    "    m = max(int(args.frac * args.num_users), 1)\n",
    "    idxs_users = np.random.choice(range(args.num_users), m, replace=False)\n",
    "\n",
    "    for idx in idxs_users:\n",
    "        local_model = LocalUpdate(args=args, dataset=train_dataset,\n",
    "                                  idxs=user_groups[idx], logger=logger)\n",
    "        w, loss = local_model.update_weights(\n",
    "            model=copy.deepcopy(global_model), global_round=epoch)\n",
    "        local_weights.append(copy.deepcopy(w))\n",
    "        local_losses.append(copy.deepcopy(loss))\n",
    "\n",
    "    # update global weights\n",
    "    global_weights = average_weights(local_weights)\n",
    "\n",
    "    # update global weights\n",
    "    global_model.load_state_dict(global_weights)\n",
    "\n",
    "    loss_avg = sum(local_losses) / len(local_losses)\n",
    "    train_loss.append(loss_avg)\n",
    "\n",
    "    # Calculate avg training accuracy over all users at every epoch\n",
    "    list_acc, list_loss = [], []\n",
    "    global_model.eval()\n",
    "    for c in range(args.num_users):\n",
    "        local_model = LocalUpdate(args=args, dataset=train_dataset,\n",
    "                                  idxs=user_groups[idx], logger=logger)\n",
    "        acc, loss = local_model.inference(model=global_model)\n",
    "        list_acc.append(acc)\n",
    "        list_loss.append(loss)\n",
    "    train_accuracy.append(sum(list_acc)/len(list_acc))\n",
    "\n",
    "    # print global training loss after every 'i' rounds\n",
    "    if (epoch+1) % print_every == 0:\n",
    "        print(f' \\nAvg Training Stats after {epoch+1} global rounds:')\n",
    "        print(f'Training Loss : {np.mean(np.array(train_loss))}')\n",
    "        print('Train Accuracy: {:.2f}% \\n'.format(100*train_accuracy[-1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20762342-1a2b-499e-a6c6-051cde869b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference after completion of training\n",
    "test_acc, test_loss = test_inference(args, global_model, test_dataset)\n",
    "\n",
    "print(f' \\n Results after {args.epochs} global rounds of training:')\n",
    "print(\"|---- Avg Train Accuracy: {:.2f}%\".format(100*train_accuracy[-1]))\n",
    "print(\"|---- Test Accuracy: {:.2f}%\".format(100*test_acc))\n",
    "\n",
    "# Saving the objects train_loss and train_accuracy:\n",
    "file_name = '../save/objects/{}_{}_{}_C[{}]_iid[{}]_E[{}]_B[{}].pkl'.\\\n",
    "    format(args.dataset, args.model, args.epochs, args.frac, args.iid,\n",
    "           args.local_ep, args.local_bs)\n",
    "\n",
    "with open(file_name, 'wb') as f:\n",
    "    pickle.dump([train_loss, train_accuracy], f)\n",
    "\n",
    "print('\\n Total Run Time: {0:0.4f}'.format(time.time()-start_time))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefad019-8e46-4098-9291-d7d3541afecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOTTING (optional)\n",
    "# import matplotlib\n",
    "# import matplotlib.pyplot as plt\n",
    "# matplotlib.use('Agg')\n",
    "\n",
    "# Plot Loss curve\n",
    "# plt.figure()\n",
    "# plt.title('Training Loss vs Communication rounds')\n",
    "# plt.plot(range(len(train_loss)), train_loss, color='r')\n",
    "# plt.ylabel('Training loss')\n",
    "# plt.xlabel('Communication Rounds')\n",
    "# plt.savefig('../save/fed_{}_{}_{}_C[{}]_iid[{}]_E[{}]_B[{}]_loss.png'.\n",
    "#             format(args.dataset, args.model, args.epochs, args.frac,\n",
    "#                    args.iid, args.local_ep, args.local_bs))\n",
    "#\n",
    "# # Plot Average Accuracy vs Communication rounds\n",
    "# plt.figure()\n",
    "# plt.title('Average Accuracy vs Communication rounds')\n",
    "# plt.plot(range(len(train_accuracy)), train_accuracy, color='k')\n",
    "# plt.ylabel('Average Accuracy')\n",
    "# plt.xlabel('Communication Rounds')\n",
    "# plt.savefig('../save/fed_{}_{}_{}_C[{}]_iid[{}]_E[{}]_B[{}]_acc.png'.\n",
    "#             format(args.dataset, args.model, args.epochs, args.frac,\n",
    "#                    args.iid, args.local_ep, args.local_bs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6dee27-c8f4-492c-b306-d5b9fef9f0c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3558dc26-1f43-4b7d-a46b-3931311f4868",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
