{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c65609f-63a2-4a64-bba2-b27a2e601e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# Python version: 3.6\n",
    "\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import argparse\n",
    "from options import args_parser\n",
    "from update_budget import LocalUpdate, test_inference\n",
    "from models import MLP, CNNMnist, CNNFashion_Mnist, CNNCifar\n",
    "from utils import get_dataset, average_weights, exp_details\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a204d617-f6ab-4248-b5f2-daad5da4ddf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here3\n",
      "here2\n",
      "\n",
      "Experimental details:\n",
      "    Model     : mlp\n",
      "    Optimizer : sgd\n",
      "    Learning  : 0.01\n",
      "    Global Rounds   : 10\n",
      "\n",
      "    Federated parameters:\n",
      "    IID\n",
      "    Fraction of users  : 0.1\n",
      "    Local Batch size   : 10\n",
      "    Local Epochs       : 10\n",
      "\n",
      "here1\n"
     ]
    }
   ],
   "source": [
    "# if __name__ == '__main__':\n",
    "start_time = time.time()\n",
    "\n",
    "# define paths\n",
    "path_project = os.path.abspath('..')\n",
    "logger = SummaryWriter('../logs')\n",
    "print('here3')\n",
    "\n",
    "args = args_parser()\n",
    "print('here2')\n",
    "exp_details(args)\n",
    "print('here1')\n",
    "device = 'cuda' if args.gpu else 'cpu'\n",
    "\n",
    "if args.gpu:\n",
    "    torch.cuda.set_device(args.gpu)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "# load dataset and user groups\n",
    "train_dataset, test_dataset, user_groups = get_dataset(args)\n",
    "\n",
    "# BUILD MODEL\n",
    "if args.model == 'cnn':\n",
    "    # Convolutional neural netork\n",
    "    if args.dataset == 'mnist':\n",
    "        global_model = CNNMnist(args=args)\n",
    "    elif args.dataset == 'fmnist':\n",
    "        global_model = CNNFashion_Mnist(args=args)\n",
    "    elif args.dataset == 'cifar':\n",
    "        global_model = CNNCifar(args=args)\n",
    "\n",
    "elif args.model == 'mlp':\n",
    "    # Multi-layer preceptron\n",
    "    img_size = train_dataset[0][0].shape\n",
    "    len_in = 1\n",
    "    for x in img_size:\n",
    "        len_in *= x\n",
    "        global_model = MLP(dim_in=len_in, dim_hidden=64,\n",
    "                           dim_out=args.num_classes)\n",
    "else:\n",
    "    print('Error: unrecognized model')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45a853d7-df64-4f21-a584-b016ba968c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_dataset.data.size())\n",
    "# print(test_dataset.data.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6025935c-591b-41f7-a9ed-c10b2f2e1060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "r1=1\n",
    "r2=10\n",
    "R=10\n",
    "a_star=args.num_users*np.exp( -1*np.power( np.math.factorial(r2)/np.math.factorial(r1-1) , 1/(r2-r1+1) ) )\n",
    "a_star=int(a_star)\n",
    "print(a_star)\n",
    "# print(type(a_star))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6a8159e-862e-4283-afc9-77bf18656e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print((train_dataset.train_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95a6b0a4-345c-41f6-b804-7848a1320ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5475e4d7-d923-47bd-9676-728367fedf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# num_users=100\n",
    "# test=np.random.exponential(scale=10, size=num_users)\n",
    "# print(test)\n",
    "# # plt.hist(test)\n",
    "# # plt.show()\n",
    "# # norm=np.random.normal(loc=1.19865, scale=1.0, size=num_users)\n",
    "# # print(norm)\n",
    "# # plt.plot(np.arange(norm.size),norm)\n",
    "# # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d0e4e3d-9c97-497f-9eba-d57302be9b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(range(100))\n",
    "# a=[2,5,1,6,2,3]\n",
    "# b=np.argsort(a, axis=- 1, kind=None, order=None)\n",
    "# print(b)\n",
    "# for i in b:\n",
    "#     print(a[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee96a88-fd93-4999-93be-d9b38b454fa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f856bc7-3dc8-408e-bf0b-8273604a57d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def gen_response_time(num_users=args.num_users):\n",
    "#     np.random.exponential(scale=1.0, size=num_users)\n",
    "# print(selected_user_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8b34fd6-cd2c-4499-a6e7-28972aef7af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (layer_input): Linear(in_features=784, out_features=64, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (layer_hidden): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " | Global Training Round : 1 |\n",
      "\n",
      "[ 7.11247006 21.15265295  8.62427793  5.42735354 14.06505168  2.08903411\n",
      " 19.48802002  0.2663009   0.59055856  3.55409316]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xin/Developer/CMPUT675_Project/src/update_budget.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(image), torch.tensor(label)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_accu: 0.5  test loss: -3.4708728939294815\n",
      "test_accu: 0.65  test loss: -5.0731604397296906\n",
      "test_accu: 0.45  test loss: -3.7890749722719193\n",
      "test_accu: 0.48333333333333334  test loss: -3.6899953931570053\n",
      "test_accu: 0.45  test loss: -4.081792190670967\n",
      "test_accu: 0.5666666666666667  test loss: -4.668956160545349\n",
      "test_accu: 0.4666666666666667  test loss: -3.730160191655159\n",
      "test_accu: 0.6333333333333333  test loss: -4.9640311151742935\n",
      "test_accu: 0.6833333333333333  test loss: -5.346763581037521\n",
      "test_accu: 0.6333333333333333  test loss: -4.746120721101761\n",
      "test_accu: 0.6833333333333333  test loss: -5.204764097929001\n",
      "test_accu: 0.5333333333333333  test loss: -4.003648012876511\n",
      "test_accu: 0.5833333333333334  test loss: -3.7486120611429214\n",
      "test_accu: 0.45  test loss: -3.390726089477539\n",
      "test_accu: 0.5166666666666667  test loss: -3.9609606713056564\n",
      "test_accu: 0.5  test loss: -3.7378707230091095\n",
      "test_accu: 0.4  test loss: -3.192061834037304\n",
      "test_accu: 0.6666666666666666  test loss: -5.160470724105835\n",
      "test_accu: 0.55  test loss: -4.2363868951797485\n",
      "test_accu: 0.5666666666666667  test loss: -4.274330377578735\n",
      "test_accu: 0.6833333333333333  test loss: -4.6862698793411255\n",
      "test_accu: 0.6166666666666667  test loss: -4.628853440284729\n",
      "test_accu: 0.4666666666666667  test loss: -3.8043294697999954\n",
      "test_accu: 0.4666666666666667  test loss: -3.733968995511532\n",
      "test_accu: 0.6333333333333333  test loss: -4.201535061001778\n",
      "test_accu: 0.5  test loss: -3.6754328459501266\n",
      "test_accu: 0.5166666666666667  test loss: -4.016590669751167\n",
      "test_accu: 0.5333333333333333  test loss: -4.240708824247122\n",
      "test_accu: 0.5833333333333334  test loss: -4.561027631163597\n",
      "test_accu: 0.65  test loss: -4.7752233147621155\n",
      "test_accu: 0.48333333333333334  test loss: -4.16306421905756\n",
      "test_accu: 0.6333333333333333  test loss: -4.571186572313309\n",
      "test_accu: 0.55  test loss: -4.328424721956253\n",
      "test_accu: 0.6  test loss: -4.258847862482071\n",
      "test_accu: 0.5833333333333334  test loss: -4.3526477217674255\n",
      "test_accu: 0.5666666666666667  test loss: -3.8615422174334526\n",
      "test_accu: 0.6666666666666666  test loss: -4.321568042039871\n",
      "test_accu: 0.6166666666666667  test loss: -4.714579910039902\n",
      "test_accu: 0.5666666666666667  test loss: -4.227822735905647\n",
      "test_accu: 0.5333333333333333  test loss: -4.135950565338135\n",
      "test_accu: 0.5  test loss: -3.9753731191158295\n",
      "test_accu: 0.5666666666666667  test loss: -4.493340402841568\n",
      "test_accu: 0.65  test loss: -4.209213748574257\n",
      "test_accu: 0.5666666666666667  test loss: -4.402089461684227\n",
      "test_accu: 0.5666666666666667  test loss: -4.0318960547447205\n",
      "test_accu: 0.7  test loss: -4.937480956315994\n",
      "test_accu: 0.65  test loss: -4.680748999118805\n",
      "test_accu: 0.6166666666666667  test loss: -3.979408383369446\n",
      "test_accu: 0.6333333333333333  test loss: -4.4839800000190735\n",
      "test_accu: 0.48333333333333334  test loss: -3.7814015299081802\n",
      "test_accu: 0.5833333333333334  test loss: -4.652679622173309\n",
      "test_accu: 0.5  test loss: -4.3362749218940735\n",
      "test_accu: 0.6  test loss: -4.732178434729576\n",
      "test_accu: 0.65  test loss: -4.2899090349674225\n",
      "test_accu: 0.4666666666666667  test loss: -3.5658084601163864\n",
      "test_accu: 0.6333333333333333  test loss: -4.790014773607254\n",
      "test_accu: 0.6  test loss: -4.6144101321697235\n",
      "test_accu: 0.6666666666666666  test loss: -4.832146197557449\n",
      "test_accu: 0.55  test loss: -4.161985620856285\n",
      "test_accu: 0.6  test loss: -4.272022441029549\n",
      "test_accu: 0.5  test loss: -3.9823446422815323\n",
      "test_accu: 0.65  test loss: -4.763149410486221\n",
      "test_accu: 0.55  test loss: -4.3437173664569855\n",
      "test_accu: 0.5166666666666667  test loss: -4.181612521409988\n",
      "test_accu: 0.35  test loss: -3.0586756467819214\n",
      "test_accu: 0.55  test loss: -4.322108790278435\n",
      "test_accu: 0.5166666666666667  test loss: -4.21852745115757\n",
      "test_accu: 0.45  test loss: -3.4023537784814835\n",
      "test_accu: 0.4166666666666667  test loss: -3.0102755427360535\n",
      "test_accu: 0.5166666666666667  test loss: -4.11105814576149\n",
      "test_accu: 0.6333333333333333  test loss: -3.5999639481306076\n",
      "test_accu: 0.5666666666666667  test loss: -4.381132543087006\n",
      "test_accu: 0.6833333333333333  test loss: -5.166849851608276\n",
      "test_accu: 0.5666666666666667  test loss: -4.007234834134579\n",
      "test_accu: 0.5  test loss: -4.207612283527851\n",
      "test_accu: 0.5666666666666667  test loss: -4.678873628377914\n",
      "test_accu: 0.65  test loss: -4.237556099891663\n",
      "test_accu: 0.6166666666666667  test loss: -4.9587757885456085\n",
      "test_accu: 0.55  test loss: -3.9582958221435547\n",
      "test_accu: 0.55  test loss: -4.007073000073433\n",
      "test_accu: 0.5166666666666667  test loss: -3.9072794131934643\n",
      "test_accu: 0.5666666666666667  test loss: -4.670529127120972\n",
      "test_accu: 0.48333333333333334  test loss: -3.798734739422798\n",
      "test_accu: 0.6333333333333333  test loss: -4.417689621448517\n",
      "test_accu: 0.6  test loss: -4.340784057974815\n",
      "test_accu: 0.55  test loss: -4.168776124715805\n",
      "test_accu: 0.4666666666666667  test loss: -4.105653375387192\n",
      "test_accu: 0.4666666666666667  test loss: -3.8625760674476624\n",
      "test_accu: 0.5833333333333334  test loss: -4.6733516454696655\n",
      "test_accu: 0.6666666666666666  test loss: -4.241505831480026\n",
      "test_accu: 0.55  test loss: -3.9408342391252518\n",
      "test_accu: 0.5833333333333334  test loss: -4.526613086462021\n",
      "test_accu: 0.5333333333333333  test loss: -4.156518395990133\n",
      "test_accu: 0.5833333333333334  test loss: -4.267785295844078\n",
      "test_accu: 0.6333333333333333  test loss: -4.698533415794373\n",
      "test_accu: 0.5166666666666667  test loss: -4.289733916521072\n",
      "test_accu: 0.43333333333333335  test loss: -3.466503605246544\n",
      "test_accu: 0.6666666666666666  test loss: -4.493925079703331\n",
      "test_accu: 0.65  test loss: -4.982503145933151\n",
      "test_accu: 0.6833333333333333  test loss: -4.822383671998978\n",
      "0.10152233508454309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████▎                                      | 1/10 [02:37<23:37, 157.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " | Global Training Round : 2 |\n",
      "\n",
      "[ 1.15091846  5.63658179 20.59300036 21.98463805  4.46217199  2.55663197\n",
      "  2.65015452  1.72375122 18.99182006  9.27434007]\n",
      "test_accu: 0.5833333333333334  test loss: -5.183255463838577\n",
      "test_accu: 0.7333333333333333  test loss: -6.70095431804657\n",
      "test_accu: 0.8166666666666667  test loss: -6.732451170682907\n",
      "test_accu: 0.6166666666666667  test loss: -5.461929619312286\n",
      "test_accu: 0.7333333333333333  test loss: -6.508720099925995\n",
      "test_accu: 0.7666666666666667  test loss: -6.917980790138245\n",
      "test_accu: 0.6  test loss: -5.174600452184677\n",
      "test_accu: 0.7166666666666667  test loss: -6.2556783854961395\n",
      "test_accu: 0.85  test loss: -7.226321160793304\n",
      "test_accu: 0.7666666666666667  test loss: -6.56078115105629\n",
      "test_accu: 0.6833333333333333  test loss: -5.8866802752017975\n",
      "test_accu: 0.7  test loss: -6.147987365722656\n",
      "test_accu: 0.7333333333333333  test loss: -6.079748302698135\n",
      "test_accu: 0.6333333333333333  test loss: -5.631695806980133\n",
      "test_accu: 0.6833333333333333  test loss: -5.680546134710312\n",
      "test_accu: 0.7333333333333333  test loss: -6.113124847412109\n",
      "test_accu: 0.5333333333333333  test loss: -4.876741088926792\n",
      "test_accu: 0.8666666666666667  test loss: -7.210581541061401\n",
      "test_accu: 0.7  test loss: -5.745002001523972\n",
      "test_accu: 0.7833333333333333  test loss: -6.5513269901275635\n",
      "test_accu: 0.7666666666666667  test loss: -6.422236979007721\n",
      "test_accu: 0.75  test loss: -6.777672976255417\n",
      "test_accu: 0.6666666666666666  test loss: -5.453227370977402\n",
      "test_accu: 0.5333333333333333  test loss: -4.657870560884476\n",
      "test_accu: 0.7  test loss: -5.788121312856674\n",
      "test_accu: 0.6333333333333333  test loss: -5.403486609458923\n",
      "test_accu: 0.6166666666666667  test loss: -5.322793364524841\n",
      "test_accu: 0.7333333333333333  test loss: -6.520818322896957\n",
      "test_accu: 0.65  test loss: -5.915785253047943\n",
      "test_accu: 0.8  test loss: -6.845424711704254\n",
      "test_accu: 0.6666666666666666  test loss: -5.998828619718552\n",
      "test_accu: 0.7666666666666667  test loss: -6.533332914113998\n",
      "test_accu: 0.7166666666666667  test loss: -6.284607142210007\n",
      "test_accu: 0.7166666666666667  test loss: -6.07254359126091\n",
      "test_accu: 0.7166666666666667  test loss: -6.2255712151527405\n",
      "test_accu: 0.6833333333333333  test loss: -5.588327556848526\n",
      "test_accu: 0.7166666666666667  test loss: -5.8415500819683075\n",
      "test_accu: 0.7166666666666667  test loss: -6.776378393173218\n",
      "test_accu: 0.7  test loss: -5.69892892241478\n",
      "test_accu: 0.75  test loss: -6.409806549549103\n",
      "test_accu: 0.7  test loss: -6.219396889209747\n",
      "test_accu: 0.6833333333333333  test loss: -6.193439364433289\n",
      "test_accu: 0.6666666666666666  test loss: -6.134696885943413\n",
      "test_accu: 0.6666666666666666  test loss: -6.090487152338028\n",
      "test_accu: 0.5833333333333334  test loss: -5.090850129723549\n",
      "test_accu: 0.8666666666666667  test loss: -7.068151950836182\n",
      "test_accu: 0.6333333333333333  test loss: -5.89856681227684\n",
      "test_accu: 0.6833333333333333  test loss: -5.794429838657379\n",
      "test_accu: 0.7  test loss: -6.212707817554474\n",
      "test_accu: 0.6833333333333333  test loss: -5.740772038698196\n",
      "test_accu: 0.65  test loss: -5.616840720176697\n",
      "test_accu: 0.7  test loss: -5.984268009662628\n",
      "test_accu: 0.6833333333333333  test loss: -5.955378323793411\n",
      "test_accu: 0.8  test loss: -6.513521105051041\n",
      "test_accu: 0.6666666666666666  test loss: -5.725510686635971\n",
      "test_accu: 0.6333333333333333  test loss: -5.733010858297348\n",
      "test_accu: 0.6666666666666666  test loss: -5.841747999191284\n",
      "test_accu: 0.6166666666666667  test loss: -5.457899481058121\n",
      "test_accu: 0.6166666666666667  test loss: -5.441622763872147\n",
      "test_accu: 0.75  test loss: -6.406183183193207\n",
      "test_accu: 0.7166666666666667  test loss: -6.151071459054947\n",
      "test_accu: 0.7166666666666667  test loss: -5.821192264556885\n",
      "test_accu: 0.7  test loss: -5.962092369794846\n",
      "test_accu: 0.6833333333333333  test loss: -5.5100094974040985\n",
      "test_accu: 0.48333333333333334  test loss: -4.455893762409687\n",
      "test_accu: 0.6333333333333333  test loss: -5.680773198604584\n",
      "test_accu: 0.7333333333333333  test loss: -6.642070263624191\n",
      "test_accu: 0.6166666666666667  test loss: -5.220115959644318\n",
      "test_accu: 0.7  test loss: -6.108637034893036\n",
      "test_accu: 0.65  test loss: -5.597103402018547\n",
      "test_accu: 0.6  test loss: -5.301756262779236\n",
      "test_accu: 0.6666666666666666  test loss: -5.79289111495018\n",
      "test_accu: 0.75  test loss: -6.574950963258743\n",
      "test_accu: 0.7666666666666667  test loss: -6.62195348739624\n",
      "test_accu: 0.6166666666666667  test loss: -5.404806047677994\n",
      "test_accu: 0.75  test loss: -6.696457058191299\n",
      "test_accu: 0.7  test loss: -5.669073477387428\n",
      "test_accu: 0.8333333333333334  test loss: -7.126649051904678\n",
      "test_accu: 0.6833333333333333  test loss: -5.402689814567566\n",
      "test_accu: 0.75  test loss: -6.59929084777832\n",
      "test_accu: 0.7  test loss: -5.94743774831295\n",
      "test_accu: 0.7666666666666667  test loss: -7.017461806535721\n",
      "test_accu: 0.6666666666666666  test loss: -5.904183715581894\n",
      "test_accu: 0.7166666666666667  test loss: -6.010570973157883\n",
      "test_accu: 0.7666666666666667  test loss: -6.1257113218307495\n",
      "test_accu: 0.65  test loss: -5.700336456298828\n",
      "test_accu: 0.6666666666666666  test loss: -5.817615121603012\n",
      "test_accu: 0.75  test loss: -6.335839092731476\n",
      "test_accu: 0.6333333333333333  test loss: -5.834465175867081\n",
      "test_accu: 0.6166666666666667  test loss: -5.530385076999664\n",
      "test_accu: 0.7166666666666667  test loss: -6.325271546840668\n",
      "test_accu: 0.7  test loss: -6.165818125009537\n",
      "test_accu: 0.7  test loss: -5.839648261666298\n",
      "test_accu: 0.7  test loss: -5.98096838593483\n",
      "test_accu: 0.7166666666666667  test loss: -6.198535084724426\n",
      "test_accu: 0.65  test loss: -6.216601878404617\n",
      "test_accu: 0.6666666666666666  test loss: -5.629505887627602\n",
      "test_accu: 0.7666666666666667  test loss: -6.58104482293129\n",
      "test_accu: 0.75  test loss: -6.638883590698242\n",
      "test_accu: 0.8  test loss: -6.79616966843605\n",
      "0.1346741759295012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████▌                                  | 2/10 [05:23<21:39, 162.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Avg Training Stats after 2 global rounds:\n",
      "Training Loss : -0.4386265010222997\n",
      "Train Accuracy: 83.33% \n",
      "\n",
      "\n",
      " | Global Training Round : 3 |\n",
      "\n",
      "[ 1.54331297  8.64137512  6.61656652  7.84086414  7.36312538  8.82942794\n",
      " 14.85631875  2.56229145  0.22377177 13.3983993 ]\n",
      "test_accu: 0.5833333333333334  test loss: -5.360638380050659\n",
      "test_accu: 0.7333333333333333  test loss: -7.029175579547882\n",
      "test_accu: 0.8166666666666667  test loss: -7.252303510904312\n",
      "test_accu: 0.6333333333333333  test loss: -5.9166848957538605\n",
      "test_accu: 0.7666666666666667  test loss: -6.783576488494873\n",
      "test_accu: 0.7666666666666667  test loss: -7.366754353046417\n",
      "test_accu: 0.6333333333333333  test loss: -5.806416869163513\n",
      "test_accu: 0.7333333333333333  test loss: -6.717102497816086\n",
      "test_accu: 0.85  test loss: -7.750788629055023\n",
      "test_accu: 0.7833333333333333  test loss: -7.154698133468628\n",
      "test_accu: 0.6833333333333333  test loss: -6.227499604225159\n",
      "test_accu: 0.7333333333333333  test loss: -6.6245008409023285\n",
      "test_accu: 0.75  test loss: -6.832609683275223\n",
      "test_accu: 0.65  test loss: -6.20747309923172\n",
      "test_accu: 0.7  test loss: -6.185030311346054\n",
      "test_accu: 0.7166666666666667  test loss: -6.543691575527191\n",
      "test_accu: 0.6  test loss: -5.465909942984581\n",
      "test_accu: 0.8333333333333334  test loss: -7.446895599365234\n",
      "test_accu: 0.6833333333333333  test loss: -5.995998203754425\n",
      "test_accu: 0.7666666666666667  test loss: -6.933134973049164\n",
      "test_accu: 0.7333333333333333  test loss: -6.8985244035720825\n",
      "test_accu: 0.7833333333333333  test loss: -7.282900005578995\n",
      "test_accu: 0.6833333333333333  test loss: -6.040234684944153\n",
      "test_accu: 0.5666666666666667  test loss: -4.986783280968666\n",
      "test_accu: 0.6833333333333333  test loss: -6.312977373600006\n",
      "test_accu: 0.6166666666666667  test loss: -5.6762266755104065\n",
      "test_accu: 0.6333333333333333  test loss: -5.735451936721802\n",
      "test_accu: 0.7  test loss: -6.865457102656364\n",
      "test_accu: 0.7  test loss: -6.47128626704216\n",
      "test_accu: 0.7833333333333333  test loss: -7.275593161582947\n",
      "test_accu: 0.7  test loss: -6.480592340230942\n",
      "test_accu: 0.7833333333333333  test loss: -6.939832031726837\n",
      "test_accu: 0.7333333333333333  test loss: -6.795264691114426\n",
      "test_accu: 0.7  test loss: -6.527362108230591\n",
      "test_accu: 0.7166666666666667  test loss: -6.594153344631195\n",
      "test_accu: 0.6833333333333333  test loss: -6.0037826001644135\n",
      "test_accu: 0.7  test loss: -6.176071614027023\n",
      "test_accu: 0.7333333333333333  test loss: -7.133734315633774\n",
      "test_accu: 0.6833333333333333  test loss: -6.231389224529266\n",
      "test_accu: 0.75  test loss: -6.955499470233917\n",
      "test_accu: 0.7166666666666667  test loss: -6.506024718284607\n",
      "test_accu: 0.6666666666666666  test loss: -6.43206313252449\n",
      "test_accu: 0.7  test loss: -6.646234408020973\n",
      "test_accu: 0.6833333333333333  test loss: -6.551872432231903\n",
      "test_accu: 0.5833333333333334  test loss: -5.631261959671974\n",
      "test_accu: 0.8833333333333333  test loss: -7.721839010715485\n",
      "test_accu: 0.6666666666666666  test loss: -6.2304253578186035\n",
      "test_accu: 0.6833333333333333  test loss: -6.22562238574028\n",
      "test_accu: 0.7333333333333333  test loss: -6.550463497638702\n",
      "test_accu: 0.6833333333333333  test loss: -6.1976344883441925\n",
      "test_accu: 0.65  test loss: -6.0159346759319305\n",
      "test_accu: 0.6833333333333333  test loss: -6.40017032623291\n",
      "test_accu: 0.7  test loss: -6.604350119829178\n",
      "test_accu: 0.8166666666666667  test loss: -7.265206307172775\n",
      "test_accu: 0.6833333333333333  test loss: -6.2873813807964325\n",
      "test_accu: 0.65  test loss: -6.048564821481705\n",
      "test_accu: 0.6166666666666667  test loss: -6.105727344751358\n",
      "test_accu: 0.65  test loss: -5.934535473585129\n",
      "test_accu: 0.6333333333333333  test loss: -5.809147864580154\n",
      "test_accu: 0.75  test loss: -6.78081750869751\n",
      "test_accu: 0.7333333333333333  test loss: -6.664820283651352\n",
      "test_accu: 0.7166666666666667  test loss: -6.307847559452057\n",
      "test_accu: 0.7  test loss: -6.3316090404987335\n",
      "test_accu: 0.7  test loss: -6.088272720575333\n",
      "test_accu: 0.5166666666666667  test loss: -4.934579819440842\n",
      "test_accu: 0.65  test loss: -6.092062056064606\n",
      "test_accu: 0.7166666666666667  test loss: -6.780513823032379\n",
      "test_accu: 0.5666666666666667  test loss: -5.400913715362549\n",
      "test_accu: 0.7166666666666667  test loss: -6.658267676830292\n",
      "test_accu: 0.65  test loss: -5.834981143474579\n",
      "test_accu: 0.6333333333333333  test loss: -5.841644540429115\n",
      "test_accu: 0.7166666666666667  test loss: -6.378925114870071\n",
      "test_accu: 0.7666666666666667  test loss: -6.915741056203842\n",
      "test_accu: 0.7666666666666667  test loss: -7.128023386001587\n",
      "test_accu: 0.5833333333333334  test loss: -5.727414101362228\n",
      "test_accu: 0.7833333333333333  test loss: -7.22710645198822\n",
      "test_accu: 0.7  test loss: -6.079161122441292\n",
      "test_accu: 0.8666666666666667  test loss: -7.708601176738739\n",
      "test_accu: 0.7  test loss: -6.18905770778656\n",
      "test_accu: 0.7666666666666667  test loss: -7.179287970066071\n",
      "test_accu: 0.7166666666666667  test loss: -6.461179077625275\n",
      "test_accu: 0.7833333333333333  test loss: -7.435560494661331\n",
      "test_accu: 0.7  test loss: -6.52476105093956\n",
      "test_accu: 0.7333333333333333  test loss: -6.631106406450272\n",
      "test_accu: 0.7666666666666667  test loss: -6.998626351356506\n",
      "test_accu: 0.6666666666666666  test loss: -6.124723732471466\n",
      "test_accu: 0.65  test loss: -6.113976091146469\n",
      "test_accu: 0.7666666666666667  test loss: -6.854704797267914\n",
      "test_accu: 0.6333333333333333  test loss: -6.201937586069107\n",
      "test_accu: 0.65  test loss: -6.066081672906876\n",
      "test_accu: 0.75  test loss: -6.765281766653061\n",
      "test_accu: 0.7166666666666667  test loss: -6.503899782896042\n",
      "test_accu: 0.7166666666666667  test loss: -6.327203661203384\n",
      "test_accu: 0.7166666666666667  test loss: -6.5724267065525055\n",
      "test_accu: 0.75  test loss: -6.777807384729385\n",
      "test_accu: 0.7  test loss: -6.621074914932251\n",
      "test_accu: 0.65  test loss: -6.124071225523949\n",
      "test_accu: 0.7666666666666667  test loss: -7.156500577926636\n",
      "test_accu: 0.75  test loss: -7.097861409187317\n",
      "test_accu: 0.7833333333333333  test loss: -7.360572278499603\n",
      "0.08384168139357424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|████████████▉                              | 3/10 [08:04<18:53, 161.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " | Global Training Round : 4 |\n",
      "\n",
      "[43.74779482  7.61004294 12.67503745  5.18053247 13.34567422  3.2597205\n",
      "  2.11671438 14.39328794 10.07009178  3.49382696]\n",
      "test_accu: 0.6  test loss: -5.6333364844322205\n",
      "test_accu: 0.75  test loss: -7.184198319911957\n",
      "test_accu: 0.8166666666666667  test loss: -7.523284584283829\n",
      "test_accu: 0.6333333333333333  test loss: -6.017296612262726\n",
      "test_accu: 0.7666666666666667  test loss: -7.2189315259456635\n",
      "test_accu: 0.7833333333333333  test loss: -7.540155827999115\n",
      "test_accu: 0.65  test loss: -6.017080843448639\n",
      "test_accu: 0.7333333333333333  test loss: -6.981809318065643\n",
      "test_accu: 0.8666666666666667  test loss: -7.885358452796936\n",
      "test_accu: 0.7833333333333333  test loss: -7.322075188159943\n",
      "test_accu: 0.7166666666666667  test loss: -6.687974572181702\n",
      "test_accu: 0.7166666666666667  test loss: -6.807094722986221\n",
      "test_accu: 0.75  test loss: -7.039672166109085\n",
      "test_accu: 0.6666666666666666  test loss: -6.432893931865692\n",
      "test_accu: 0.6833333333333333  test loss: -6.319324761629105\n",
      "test_accu: 0.7333333333333333  test loss: -6.71392896771431\n",
      "test_accu: 0.5833333333333334  test loss: -5.57172130048275\n",
      "test_accu: 0.85  test loss: -7.732864439487457\n",
      "test_accu: 0.6833333333333333  test loss: -6.275388985872269\n",
      "test_accu: 0.7666666666666667  test loss: -7.097986549139023\n",
      "test_accu: 0.7666666666666667  test loss: -7.1420280039310455\n",
      "test_accu: 0.7833333333333333  test loss: -7.481267869472504\n",
      "test_accu: 0.6666666666666666  test loss: -6.171030163764954\n",
      "test_accu: 0.55  test loss: -5.220932289958\n",
      "test_accu: 0.7166666666666667  test loss: -6.419710755348206\n",
      "test_accu: 0.6333333333333333  test loss: -5.932293176651001\n",
      "test_accu: 0.65  test loss: -6.012993305921555\n",
      "test_accu: 0.7333333333333333  test loss: -7.033088818192482\n",
      "test_accu: 0.7  test loss: -6.6254547238349915\n",
      "test_accu: 0.8  test loss: -7.503975927829742\n",
      "test_accu: 0.7333333333333333  test loss: -6.719556361436844\n",
      "test_accu: 0.75  test loss: -7.165810406208038\n",
      "test_accu: 0.75  test loss: -7.075378984212875\n",
      "test_accu: 0.7166666666666667  test loss: -6.734918415546417\n",
      "test_accu: 0.7333333333333333  test loss: -6.884126961231232\n",
      "test_accu: 0.6666666666666666  test loss: -6.045839846134186\n",
      "test_accu: 0.7166666666666667  test loss: -6.329233407974243\n",
      "test_accu: 0.75  test loss: -7.276494145393372\n",
      "test_accu: 0.7166666666666667  test loss: -6.462038487195969\n",
      "test_accu: 0.75  test loss: -7.18062561750412\n",
      "test_accu: 0.7166666666666667  test loss: -6.596490681171417\n",
      "test_accu: 0.6833333333333333  test loss: -6.586934387683868\n",
      "test_accu: 0.7333333333333333  test loss: -6.871456190943718\n",
      "test_accu: 0.6833333333333333  test loss: -6.689750701189041\n",
      "test_accu: 0.6  test loss: -5.899442821741104\n",
      "test_accu: 0.8166666666666667  test loss: -7.849132537841797\n",
      "test_accu: 0.6666666666666666  test loss: -6.262766450643539\n",
      "test_accu: 0.6833333333333333  test loss: -6.493957877159119\n",
      "test_accu: 0.75  test loss: -6.92251843214035\n",
      "test_accu: 0.7  test loss: -6.42672361433506\n",
      "test_accu: 0.6333333333333333  test loss: -6.14609931409359\n",
      "test_accu: 0.6666666666666666  test loss: -6.396546512842178\n",
      "test_accu: 0.6833333333333333  test loss: -6.652826577425003\n",
      "test_accu: 0.8333333333333334  test loss: -7.492436975240707\n",
      "test_accu: 0.6666666666666666  test loss: -6.437198787927628\n",
      "test_accu: 0.6666666666666666  test loss: -6.222553849220276\n",
      "test_accu: 0.6333333333333333  test loss: -6.259135603904724\n",
      "test_accu: 0.6166666666666667  test loss: -5.931549459695816\n",
      "test_accu: 0.65  test loss: -5.944744139909744\n",
      "test_accu: 0.75  test loss: -7.050480246543884\n",
      "test_accu: 0.7333333333333333  test loss: -6.865505784749985\n",
      "test_accu: 0.7  test loss: -6.559755116701126\n",
      "test_accu: 0.6666666666666666  test loss: -6.375139802694321\n",
      "test_accu: 0.7  test loss: -6.4148310124874115\n",
      "test_accu: 0.5333333333333333  test loss: -5.222837284207344\n",
      "test_accu: 0.65  test loss: -6.181378543376923\n",
      "test_accu: 0.7166666666666667  test loss: -6.92122220993042\n",
      "test_accu: 0.5833333333333334  test loss: -5.589966505765915\n",
      "test_accu: 0.7333333333333333  test loss: -6.9566493928432465\n",
      "test_accu: 0.6666666666666666  test loss: -6.16514652967453\n",
      "test_accu: 0.65  test loss: -6.126370787620544\n",
      "test_accu: 0.7  test loss: -6.562830418348312\n",
      "test_accu: 0.7666666666666667  test loss: -7.357541084289551\n",
      "test_accu: 0.7833333333333333  test loss: -7.2218658328056335\n",
      "test_accu: 0.6166666666666667  test loss: -5.865043610334396\n",
      "test_accu: 0.7833333333333333  test loss: -7.5286014676094055\n",
      "test_accu: 0.7  test loss: -6.312928810715675\n",
      "test_accu: 0.8666666666666667  test loss: -7.988268435001373\n",
      "test_accu: 0.6833333333333333  test loss: -6.493923664093018\n",
      "test_accu: 0.8  test loss: -7.399161368608475\n",
      "test_accu: 0.7166666666666667  test loss: -6.821037977933884\n",
      "test_accu: 0.8  test loss: -7.674966782331467\n",
      "test_accu: 0.6833333333333333  test loss: -6.692428320646286\n",
      "test_accu: 0.7333333333333333  test loss: -6.8924753069877625\n",
      "test_accu: 0.7666666666666667  test loss: -7.083826929330826\n",
      "test_accu: 0.6666666666666666  test loss: -6.299126297235489\n",
      "test_accu: 0.6666666666666666  test loss: -6.239757537841797\n",
      "test_accu: 0.7666666666666667  test loss: -7.068793386220932\n",
      "test_accu: 0.6666666666666666  test loss: -6.424881994724274\n",
      "test_accu: 0.6166666666666667  test loss: -6.099131941795349\n",
      "test_accu: 0.7333333333333333  test loss: -7.109108597040176\n",
      "test_accu: 0.7  test loss: -6.582481354475021\n",
      "test_accu: 0.7166666666666667  test loss: -6.574359953403473\n",
      "test_accu: 0.7166666666666667  test loss: -6.627970367670059\n",
      "test_accu: 0.75  test loss: -6.930273860692978\n",
      "test_accu: 0.7  test loss: -6.751462548971176\n",
      "test_accu: 0.65  test loss: -6.251855283975601\n",
      "test_accu: 0.7666666666666667  test loss: -7.4221309423446655\n",
      "test_accu: 0.7833333333333333  test loss: -7.28228422999382\n",
      "test_accu: 0.7833333333333333  test loss: -7.635799825191498\n",
      "0.027146856187738794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████▏                         | 4/10 [10:49<16:19, 163.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Avg Training Stats after 4 global rounds:\n",
      "Training Loss : -0.5507394270847532\n",
      "Train Accuracy: 83.33% \n",
      "\n",
      "\n",
      " | Global Training Round : 5 |\n",
      "\n",
      "[1.76613320e+01 4.22837047e+01 2.45183921e+00 1.08852551e+00\n",
      " 9.46194027e-03 4.94236538e+00 9.70311171e+00 1.06835452e+01\n",
      " 9.11188533e+00 7.19075358e+00]\n",
      "test_accu: 0.65  test loss: -5.89381816983223\n",
      "test_accu: 0.7666666666666667  test loss: -7.289741277694702\n",
      "test_accu: 0.8  test loss: -7.609659105539322\n",
      "test_accu: 0.6333333333333333  test loss: -6.133683890104294\n",
      "test_accu: 0.8  test loss: -7.372604459524155\n",
      "test_accu: 0.7833333333333333  test loss: -7.6898273229599\n",
      "test_accu: 0.6833333333333333  test loss: -6.165643662214279\n",
      "test_accu: 0.7166666666666667  test loss: -6.896374017000198\n",
      "test_accu: 0.8666666666666667  test loss: -7.995507299900055\n",
      "test_accu: 0.8  test loss: -7.493115305900574\n",
      "test_accu: 0.7166666666666667  test loss: -6.797469854354858\n",
      "test_accu: 0.7  test loss: -6.787992894649506\n",
      "test_accu: 0.7166666666666667  test loss: -6.952883154153824\n",
      "test_accu: 0.6666666666666666  test loss: -6.538573741912842\n",
      "test_accu: 0.7  test loss: -6.533534020185471\n",
      "test_accu: 0.7333333333333333  test loss: -6.860353201627731\n",
      "test_accu: 0.6  test loss: -5.7586879432201385\n",
      "test_accu: 0.8166666666666667  test loss: -7.730495810508728\n",
      "test_accu: 0.6666666666666666  test loss: -6.2771696746349335\n",
      "test_accu: 0.7666666666666667  test loss: -7.136674851179123\n",
      "test_accu: 0.7666666666666667  test loss: -7.3104303777217865\n",
      "test_accu: 0.8166666666666667  test loss: -7.654675275087357\n",
      "test_accu: 0.6666666666666666  test loss: -6.333012938499451\n",
      "test_accu: 0.5333333333333333  test loss: -5.142974190413952\n",
      "test_accu: 0.7  test loss: -6.4724942445755005\n",
      "test_accu: 0.6166666666666667  test loss: -5.900129973888397\n",
      "test_accu: 0.65  test loss: -6.133855223655701\n",
      "test_accu: 0.75  test loss: -7.151986703276634\n",
      "test_accu: 0.7  test loss: -6.559224843978882\n",
      "test_accu: 0.8166666666666667  test loss: -7.713654577732086\n",
      "test_accu: 0.7333333333333333  test loss: -6.785754233598709\n",
      "test_accu: 0.7333333333333333  test loss: -7.165661633014679\n",
      "test_accu: 0.75  test loss: -7.116820603609085\n",
      "test_accu: 0.7  test loss: -6.764651834964752\n",
      "test_accu: 0.7333333333333333  test loss: -7.073467344045639\n",
      "test_accu: 0.6666666666666666  test loss: -6.372954785823822\n",
      "test_accu: 0.7166666666666667  test loss: -6.537878453731537\n",
      "test_accu: 0.75  test loss: -7.265933066606522\n",
      "test_accu: 0.7  test loss: -6.630822718143463\n",
      "test_accu: 0.75  test loss: -7.3473920822143555\n",
      "test_accu: 0.7166666666666667  test loss: -6.774416118860245\n",
      "test_accu: 0.7  test loss: -6.70028293132782\n",
      "test_accu: 0.7  test loss: -6.880061432719231\n",
      "test_accu: 0.7  test loss: -6.726465314626694\n",
      "test_accu: 0.5833333333333334  test loss: -5.962222129106522\n",
      "test_accu: 0.8833333333333333  test loss: -8.159591138362885\n",
      "test_accu: 0.6666666666666666  test loss: -6.3643583953380585\n",
      "test_accu: 0.6833333333333333  test loss: -6.606582045555115\n",
      "test_accu: 0.7333333333333333  test loss: -6.84336394071579\n",
      "test_accu: 0.7  test loss: -6.548463627696037\n",
      "test_accu: 0.7  test loss: -6.248864397406578\n",
      "test_accu: 0.6666666666666666  test loss: -6.592940479516983\n",
      "test_accu: 0.7  test loss: -6.7492296397686005\n",
      "test_accu: 0.85  test loss: -7.713769048452377\n",
      "test_accu: 0.6666666666666666  test loss: -6.5448660254478455\n",
      "test_accu: 0.65  test loss: -6.393332839012146\n",
      "test_accu: 0.6333333333333333  test loss: -6.301632940769196\n",
      "test_accu: 0.6166666666666667  test loss: -6.084605634212494\n",
      "test_accu: 0.6666666666666666  test loss: -6.0603784918785095\n",
      "test_accu: 0.75  test loss: -7.029884189367294\n",
      "test_accu: 0.75  test loss: -7.0558575093746185\n",
      "test_accu: 0.7  test loss: -6.561034470796585\n",
      "test_accu: 0.7166666666666667  test loss: -6.642708480358124\n",
      "test_accu: 0.7166666666666667  test loss: -6.565247178077698\n",
      "test_accu: 0.6  test loss: -5.549881353974342\n",
      "test_accu: 0.65  test loss: -6.159219443798065\n",
      "test_accu: 0.7166666666666667  test loss: -7.21051812171936\n",
      "test_accu: 0.5833333333333334  test loss: -5.58390611410141\n",
      "test_accu: 0.7333333333333333  test loss: -7.06932720541954\n",
      "test_accu: 0.6666666666666666  test loss: -6.185100167989731\n",
      "test_accu: 0.6333333333333333  test loss: -6.381464898586273\n",
      "test_accu: 0.6833333333333333  test loss: -6.674845635890961\n",
      "test_accu: 0.7666666666666667  test loss: -7.271459758281708\n",
      "test_accu: 0.7833333333333333  test loss: -7.451164901256561\n",
      "test_accu: 0.6  test loss: -5.959934115409851\n",
      "test_accu: 0.7833333333333333  test loss: -7.596772193908691\n",
      "test_accu: 0.7166666666666667  test loss: -6.574979946017265\n",
      "test_accu: 0.8333333333333334  test loss: -8.031718492507935\n",
      "test_accu: 0.7  test loss: -6.655959486961365\n",
      "test_accu: 0.8166666666666667  test loss: -7.664115071296692\n",
      "test_accu: 0.7166666666666667  test loss: -6.90277898311615\n",
      "test_accu: 0.8166666666666667  test loss: -7.6666558384895325\n",
      "test_accu: 0.7  test loss: -7.004811853170395\n",
      "test_accu: 0.75  test loss: -7.112624704837799\n",
      "test_accu: 0.7833333333333333  test loss: -7.368504196405411\n",
      "test_accu: 0.6833333333333333  test loss: -6.473000258207321\n",
      "test_accu: 0.65  test loss: -6.268809378147125\n",
      "test_accu: 0.7666666666666667  test loss: -7.261786043643951\n",
      "test_accu: 0.65  test loss: -6.494898855686188\n",
      "test_accu: 0.6166666666666667  test loss: -6.2506144642829895\n",
      "test_accu: 0.7333333333333333  test loss: -7.252674907445908\n",
      "test_accu: 0.7  test loss: -6.764645427465439\n",
      "test_accu: 0.7166666666666667  test loss: -6.714926570653915\n",
      "test_accu: 0.7166666666666667  test loss: -6.881127238273621\n",
      "test_accu: 0.75  test loss: -7.092537075281143\n",
      "test_accu: 0.6833333333333333  test loss: -6.636516362428665\n",
      "test_accu: 0.65  test loss: -6.270450547337532\n",
      "test_accu: 0.75  test loss: -7.3818694949150085\n",
      "test_accu: 0.7833333333333333  test loss: -7.308803766965866\n",
      "test_accu: 0.7833333333333333  test loss: -7.520444095134735\n",
      "0.009461940273196232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████▌                     | 5/10 [13:31<13:33, 162.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " | Global Training Round : 6 |\n",
      "\n",
      "[2.4226594  1.95873895 1.8020325  9.65860689 3.87965351 0.52854212\n",
      " 6.95276141 3.15922502 1.43230087 2.8208835 ]\n",
      "test_accu: 0.65  test loss: -6.013397008180618\n",
      "test_accu: 0.7666666666666667  test loss: -7.505669414997101\n",
      "test_accu: 0.8  test loss: -7.683831542730331\n",
      "test_accu: 0.6166666666666667  test loss: -6.1223964393138885\n",
      "test_accu: 0.7833333333333333  test loss: -7.5111196637153625\n",
      "test_accu: 0.7833333333333333  test loss: -7.652912259101868\n",
      "test_accu: 0.6833333333333333  test loss: -6.149461567401886\n",
      "test_accu: 0.7166666666666667  test loss: -7.071334213018417\n",
      "test_accu: 0.8666666666666667  test loss: -8.152597069740295\n",
      "test_accu: 0.8  test loss: -7.645518511533737\n",
      "test_accu: 0.7166666666666667  test loss: -6.833880007266998\n",
      "test_accu: 0.7  test loss: -6.6528675854206085\n",
      "test_accu: 0.7333333333333333  test loss: -7.121161490678787\n",
      "test_accu: 0.6833333333333333  test loss: -6.57538229227066\n",
      "test_accu: 0.7166666666666667  test loss: -6.645676493644714\n",
      "test_accu: 0.7166666666666667  test loss: -6.83485671877861\n",
      "test_accu: 0.6166666666666667  test loss: -5.807599142193794\n",
      "test_accu: 0.7833333333333333  test loss: -7.5424628257751465\n",
      "test_accu: 0.6666666666666666  test loss: -6.438205003738403\n",
      "test_accu: 0.75  test loss: -7.1413567662239075\n",
      "test_accu: 0.75  test loss: -7.276213586330414\n",
      "test_accu: 0.8  test loss: -7.630335420370102\n",
      "test_accu: 0.6666666666666666  test loss: -6.340442955493927\n",
      "test_accu: 0.55  test loss: -5.201429218053818\n",
      "test_accu: 0.6666666666666666  test loss: -6.382641792297363\n",
      "test_accu: 0.6166666666666667  test loss: -5.923923194408417\n",
      "test_accu: 0.65  test loss: -6.1983281672000885\n",
      "test_accu: 0.7666666666666667  test loss: -7.276130497455597\n",
      "test_accu: 0.7  test loss: -6.589923411607742\n",
      "test_accu: 0.8166666666666667  test loss: -7.813680171966553\n",
      "test_accu: 0.7333333333333333  test loss: -6.977279633283615\n",
      "test_accu: 0.7666666666666667  test loss: -7.255542188882828\n",
      "test_accu: 0.75  test loss: -7.2851811945438385\n",
      "test_accu: 0.7166666666666667  test loss: -6.882602155208588\n",
      "test_accu: 0.7333333333333333  test loss: -7.122734725475311\n",
      "test_accu: 0.6666666666666666  test loss: -6.1798001527786255\n",
      "test_accu: 0.6833333333333333  test loss: -6.500411033630371\n",
      "test_accu: 0.7666666666666667  test loss: -7.313670843839645\n",
      "test_accu: 0.7166666666666667  test loss: -6.746738970279694\n",
      "test_accu: 0.75  test loss: -7.483671307563782\n",
      "test_accu: 0.7166666666666667  test loss: -6.8625830709934235\n",
      "test_accu: 0.6666666666666666  test loss: -6.661246418952942\n",
      "test_accu: 0.7  test loss: -6.968343965709209\n",
      "test_accu: 0.6833333333333333  test loss: -6.747133195400238\n",
      "test_accu: 0.6  test loss: -6.019787698984146\n",
      "test_accu: 0.8666666666666667  test loss: -8.210700273513794\n",
      "test_accu: 0.65  test loss: -6.399367690086365\n",
      "test_accu: 0.7  test loss: -6.861986041069031\n",
      "test_accu: 0.7166666666666667  test loss: -6.874509304761887\n",
      "test_accu: 0.7  test loss: -6.644223153591156\n",
      "test_accu: 0.6666666666666666  test loss: -6.279988348484039\n",
      "test_accu: 0.6666666666666666  test loss: -6.653171181678772\n",
      "test_accu: 0.7  test loss: -6.887860864400864\n",
      "test_accu: 0.8333333333333334  test loss: -7.797491043806076\n",
      "test_accu: 0.6833333333333333  test loss: -6.8111593425273895\n",
      "test_accu: 0.65  test loss: -6.459621161222458\n",
      "test_accu: 0.6666666666666666  test loss: -6.376908659934998\n",
      "test_accu: 0.65  test loss: -6.187473833560944\n",
      "test_accu: 0.65  test loss: -6.258192390203476\n",
      "test_accu: 0.75  test loss: -7.0358854830265045\n",
      "test_accu: 0.7666666666666667  test loss: -7.098785191774368\n",
      "test_accu: 0.6666666666666666  test loss: -6.577971696853638\n",
      "test_accu: 0.6833333333333333  test loss: -6.6788351237773895\n",
      "test_accu: 0.6833333333333333  test loss: -6.530921071767807\n",
      "test_accu: 0.55  test loss: -5.336465910077095\n",
      "test_accu: 0.65  test loss: -6.1855732798576355\n",
      "test_accu: 0.75  test loss: -7.2020909786224365\n",
      "test_accu: 0.5666666666666667  test loss: -5.599841386079788\n",
      "test_accu: 0.75  test loss: -7.185453563928604\n",
      "test_accu: 0.6666666666666666  test loss: -6.286302864551544\n",
      "test_accu: 0.6666666666666666  test loss: -6.553194344043732\n",
      "test_accu: 0.6833333333333333  test loss: -6.742264002561569\n",
      "test_accu: 0.75  test loss: -7.277365446090698\n",
      "test_accu: 0.7833333333333333  test loss: -7.565653085708618\n",
      "test_accu: 0.6  test loss: -5.971593022346497\n",
      "test_accu: 0.7833333333333333  test loss: -7.687656044960022\n",
      "test_accu: 0.6833333333333333  test loss: -6.634127199649811\n",
      "test_accu: 0.8666666666666667  test loss: -8.16842234134674\n",
      "test_accu: 0.7  test loss: -6.755068480968475\n",
      "test_accu: 0.8  test loss: -7.766685366630554\n",
      "test_accu: 0.75  test loss: -7.195355623960495\n",
      "test_accu: 0.8  test loss: -7.781929582357407\n",
      "test_accu: 0.7  test loss: -7.0323173105716705\n",
      "test_accu: 0.7666666666666667  test loss: -7.257445871829987\n",
      "test_accu: 0.8  test loss: -7.483148604631424\n",
      "test_accu: 0.6833333333333333  test loss: -6.632402688264847\n",
      "test_accu: 0.65  test loss: -6.271343946456909\n",
      "test_accu: 0.75  test loss: -7.244085848331451\n",
      "test_accu: 0.6666666666666666  test loss: -6.5860466957092285\n",
      "test_accu: 0.65  test loss: -6.382496923208237\n",
      "test_accu: 0.7333333333333333  test loss: -7.298329591751099\n",
      "test_accu: 0.7  test loss: -6.7284862995147705\n",
      "test_accu: 0.7166666666666667  test loss: -6.792379260063171\n",
      "test_accu: 0.7166666666666667  test loss: -6.903177797794342\n",
      "test_accu: 0.7666666666666667  test loss: -7.235503315925598\n",
      "test_accu: 0.6833333333333333  test loss: -6.697564482688904\n",
      "test_accu: 0.65  test loss: -6.387192726135254\n",
      "test_accu: 0.7666666666666667  test loss: -7.511487543582916\n",
      "test_accu: 0.7666666666666667  test loss: -7.447347313165665\n",
      "test_accu: 0.8  test loss: -7.766003459692001\n",
      "0.0012737581965578918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████████████████████████▊                 | 6/10 [16:41<11:27, 171.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Avg Training Stats after 6 global rounds:\n",
      "Training Loss : -0.5993063307927545\n",
      "Train Accuracy: 83.33% \n",
      "\n",
      "\n",
      " | Global Training Round : 7 |\n",
      "\n",
      "[15.40712568 13.03452857 12.58638209  0.84980911 31.75595712 15.77370724\n",
      "  8.33014809 13.37705032  5.36746632 41.28433569]\n",
      "test_accu: 0.6666666666666666  test loss: -6.177459716796875\n",
      "test_accu: 0.7666666666666667  test loss: -7.449553966522217\n",
      "test_accu: 0.8  test loss: -7.800063639879227\n",
      "test_accu: 0.6166666666666667  test loss: -6.101841956377029\n",
      "test_accu: 0.7666666666666667  test loss: -7.3975575268268585\n",
      "test_accu: 0.8  test loss: -7.723633348941803\n",
      "test_accu: 0.6833333333333333  test loss: -6.291436046361923\n",
      "test_accu: 0.7  test loss: -6.964148461818695\n",
      "test_accu: 0.8666666666666667  test loss: -8.240756690502167\n",
      "test_accu: 0.8  test loss: -7.688158065080643\n",
      "test_accu: 0.7166666666666667  test loss: -6.849399536848068\n",
      "test_accu: 0.7166666666666667  test loss: -6.904661625623703\n",
      "test_accu: 0.7166666666666667  test loss: -7.001519471406937\n",
      "test_accu: 0.6666666666666666  test loss: -6.619128853082657\n",
      "test_accu: 0.7  test loss: -6.683771759271622\n",
      "test_accu: 0.7  test loss: -6.732710212469101\n",
      "test_accu: 0.6  test loss: -5.877474710345268\n",
      "test_accu: 0.8166666666666667  test loss: -7.770672380924225\n",
      "test_accu: 0.6833333333333333  test loss: -6.48347669839859\n",
      "test_accu: 0.75  test loss: -7.224186718463898\n",
      "test_accu: 0.7666666666666667  test loss: -7.415236204862595\n",
      "test_accu: 0.8166666666666667  test loss: -7.744697988033295\n",
      "test_accu: 0.6666666666666666  test loss: -6.371197134256363\n",
      "test_accu: 0.5833333333333334  test loss: -5.512484531849623\n",
      "test_accu: 0.6666666666666666  test loss: -6.546625435352325\n",
      "test_accu: 0.6166666666666667  test loss: -6.059351444244385\n",
      "test_accu: 0.6166666666666667  test loss: -6.037289381027222\n",
      "test_accu: 0.7666666666666667  test loss: -7.3275888711214066\n",
      "test_accu: 0.6833333333333333  test loss: -6.620746076107025\n",
      "test_accu: 0.8166666666666667  test loss: -7.9203813672065735\n",
      "test_accu: 0.7166666666666667  test loss: -6.830875664949417\n",
      "test_accu: 0.7666666666666667  test loss: -7.415754050016403\n",
      "test_accu: 0.7666666666666667  test loss: -7.339240074157715\n",
      "test_accu: 0.7166666666666667  test loss: -6.996668636798859\n",
      "test_accu: 0.7166666666666667  test loss: -7.083274841308594\n",
      "test_accu: 0.6666666666666666  test loss: -6.451689153909683\n",
      "test_accu: 0.7  test loss: -6.608019053936005\n",
      "test_accu: 0.7666666666666667  test loss: -7.448946595191956\n",
      "test_accu: 0.7166666666666667  test loss: -6.800311416387558\n",
      "test_accu: 0.75  test loss: -7.493548333644867\n",
      "test_accu: 0.7  test loss: -6.832754015922546\n",
      "test_accu: 0.65  test loss: -6.65323793888092\n",
      "test_accu: 0.7166666666666667  test loss: -7.019690737128258\n",
      "test_accu: 0.6833333333333333  test loss: -6.7883442640304565\n",
      "test_accu: 0.6  test loss: -6.05546173453331\n",
      "test_accu: 0.8666666666666667  test loss: -8.28184461593628\n",
      "test_accu: 0.65  test loss: -6.383946359157562\n",
      "test_accu: 0.7166666666666667  test loss: -7.002925336360931\n",
      "test_accu: 0.75  test loss: -6.975739538669586\n",
      "test_accu: 0.6833333333333333  test loss: -6.590963736176491\n",
      "test_accu: 0.6833333333333333  test loss: -6.333050534129143\n",
      "test_accu: 0.6666666666666666  test loss: -6.63803568482399\n",
      "test_accu: 0.7166666666666667  test loss: -6.957262217998505\n",
      "test_accu: 0.8333333333333334  test loss: -7.916489452123642\n",
      "test_accu: 0.7  test loss: -6.848286718130112\n",
      "test_accu: 0.65  test loss: -6.472469866275787\n",
      "test_accu: 0.7  test loss: -6.648092418909073\n",
      "test_accu: 0.6333333333333333  test loss: -6.1732635498046875\n",
      "test_accu: 0.6666666666666666  test loss: -6.348913609981537\n",
      "test_accu: 0.7333333333333333  test loss: -7.058773159980774\n",
      "test_accu: 0.75  test loss: -7.124914318323135\n",
      "test_accu: 0.6833333333333333  test loss: -6.585498064756393\n",
      "test_accu: 0.6833333333333333  test loss: -6.568818509578705\n",
      "test_accu: 0.7166666666666667  test loss: -6.786404311656952\n",
      "test_accu: 0.6166666666666667  test loss: -5.771358504891396\n",
      "test_accu: 0.65  test loss: -6.360043674707413\n",
      "test_accu: 0.7166666666666667  test loss: -7.217289716005325\n",
      "test_accu: 0.5666666666666667  test loss: -5.674107670783997\n",
      "test_accu: 0.75  test loss: -7.360073983669281\n",
      "test_accu: 0.6666666666666666  test loss: -6.359271317720413\n",
      "test_accu: 0.6833333333333333  test loss: -6.688705265522003\n",
      "test_accu: 0.6833333333333333  test loss: -6.7833453714847565\n",
      "test_accu: 0.7666666666666667  test loss: -7.407813876867294\n",
      "test_accu: 0.7666666666666667  test loss: -7.509681046009064\n",
      "test_accu: 0.6  test loss: -6.0262355506420135\n",
      "test_accu: 0.7833333333333333  test loss: -7.745833873748779\n",
      "test_accu: 0.7  test loss: -6.73211669921875\n",
      "test_accu: 0.85  test loss: -8.192546844482422\n",
      "test_accu: 0.6833333333333333  test loss: -6.789357155561447\n",
      "test_accu: 0.8  test loss: -7.726026892662048\n",
      "test_accu: 0.7333333333333333  test loss: -7.034635633230209\n",
      "test_accu: 0.8166666666666667  test loss: -7.846449255943298\n",
      "test_accu: 0.7166666666666667  test loss: -7.102197080850601\n",
      "test_accu: 0.7666666666666667  test loss: -7.374442934989929\n",
      "test_accu: 0.7833333333333333  test loss: -7.401293486356735\n",
      "test_accu: 0.7  test loss: -6.662244617938995\n",
      "test_accu: 0.6666666666666666  test loss: -6.377414971590042\n",
      "test_accu: 0.7666666666666667  test loss: -7.334215611219406\n",
      "test_accu: 0.6666666666666666  test loss: -6.666473090648651\n",
      "test_accu: 0.6666666666666666  test loss: -6.520847141742706\n",
      "test_accu: 0.7666666666666667  test loss: -7.438131898641586\n",
      "test_accu: 0.7166666666666667  test loss: -6.812745183706284\n",
      "test_accu: 0.7  test loss: -6.7777723371982574\n",
      "test_accu: 0.7166666666666667  test loss: -6.997809797525406\n",
      "test_accu: 0.7666666666666667  test loss: -7.352613747119904\n",
      "test_accu: 0.6833333333333333  test loss: -6.6553773283958435\n",
      "test_accu: 0.65  test loss: -6.400529861450195\n",
      "test_accu: 0.75  test loss: -7.506570100784302\n",
      "test_accu: 0.7666666666666667  test loss: -7.40290841460228\n",
      "test_accu: 0.8166666666666667  test loss: -7.880524963140488\n",
      "0.18869361050291664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████████████████████████████             | 7/10 [19:31<08:34, 171.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " | Global Training Round : 8 |\n",
      "\n",
      "[ 5.01612472 11.29658337  4.07095751  0.62267027  3.22622648  7.45243277\n",
      "  0.21235512  9.82542709 15.54132321  1.04197444]\n",
      "test_accu: 0.6666666666666666  test loss: -6.237669855356216\n",
      "test_accu: 0.75  test loss: -7.436632812023163\n",
      "test_accu: 0.8166666666666667  test loss: -7.910315841436386\n",
      "test_accu: 0.65  test loss: -6.187812238931656\n",
      "test_accu: 0.8  test loss: -7.555507332086563\n",
      "test_accu: 0.7833333333333333  test loss: -7.667226850986481\n",
      "test_accu: 0.6833333333333333  test loss: -6.413543194532394\n",
      "test_accu: 0.7166666666666667  test loss: -7.093661338090897\n",
      "test_accu: 0.8666666666666667  test loss: -8.234366834163666\n",
      "test_accu: 0.7833333333333333  test loss: -7.576459437608719\n",
      "test_accu: 0.7333333333333333  test loss: -7.050350487232208\n",
      "test_accu: 0.6833333333333333  test loss: -6.900763541460037\n",
      "test_accu: 0.7166666666666667  test loss: -7.035488098859787\n",
      "test_accu: 0.6833333333333333  test loss: -6.66957688331604\n",
      "test_accu: 0.7166666666666667  test loss: -6.797849863767624\n",
      "test_accu: 0.7166666666666667  test loss: -6.8805756866931915\n",
      "test_accu: 0.6  test loss: -5.856769725680351\n",
      "test_accu: 0.8  test loss: -7.726385593414307\n",
      "test_accu: 0.6333333333333333  test loss: -6.302870810031891\n",
      "test_accu: 0.7333333333333333  test loss: -7.149867802858353\n",
      "test_accu: 0.7666666666666667  test loss: -7.460298627614975\n",
      "test_accu: 0.8  test loss: -7.711215943098068\n",
      "test_accu: 0.6666666666666666  test loss: -6.423951178789139\n",
      "test_accu: 0.5666666666666667  test loss: -5.3480955976992846\n",
      "test_accu: 0.7166666666666667  test loss: -6.628570050001144\n",
      "test_accu: 0.6166666666666667  test loss: -6.0795294642448425\n",
      "test_accu: 0.6333333333333333  test loss: -6.149604916572571\n",
      "test_accu: 0.75  test loss: -7.346616104245186\n",
      "test_accu: 0.7  test loss: -6.777104407548904\n",
      "test_accu: 0.8333333333333334  test loss: -8.027177453041077\n",
      "test_accu: 0.7333333333333333  test loss: -6.8591369688510895\n",
      "test_accu: 0.7666666666666667  test loss: -7.409853279590607\n",
      "test_accu: 0.75  test loss: -7.378891408443451\n",
      "test_accu: 0.7166666666666667  test loss: -6.95238322019577\n",
      "test_accu: 0.75  test loss: -7.244407087564468\n",
      "test_accu: 0.6666666666666666  test loss: -6.3812165558338165\n",
      "test_accu: 0.7  test loss: -6.674890398979187\n",
      "test_accu: 0.7666666666666667  test loss: -7.43189200758934\n",
      "test_accu: 0.7166666666666667  test loss: -6.913715571165085\n",
      "test_accu: 0.7666666666666667  test loss: -7.598298788070679\n",
      "test_accu: 0.7  test loss: -6.776475250720978\n",
      "test_accu: 0.7333333333333333  test loss: -6.799245476722717\n",
      "test_accu: 0.7333333333333333  test loss: -7.115694746375084\n",
      "test_accu: 0.6833333333333333  test loss: -6.751964330673218\n",
      "test_accu: 0.5833333333333334  test loss: -5.998576104640961\n",
      "test_accu: 0.85  test loss: -8.254960238933563\n",
      "test_accu: 0.65  test loss: -6.391051411628723\n",
      "test_accu: 0.7  test loss: -6.784734755754471\n",
      "test_accu: 0.75  test loss: -7.045745939016342\n",
      "test_accu: 0.65  test loss: -6.532186850905418\n",
      "test_accu: 0.6666666666666666  test loss: -6.330102756619453\n",
      "test_accu: 0.6666666666666666  test loss: -6.611578732728958\n",
      "test_accu: 0.7166666666666667  test loss: -7.0159832537174225\n",
      "test_accu: 0.8333333333333334  test loss: -7.94212332367897\n",
      "test_accu: 0.7  test loss: -6.911991119384766\n",
      "test_accu: 0.6666666666666666  test loss: -6.492655992507935\n",
      "test_accu: 0.6833333333333333  test loss: -6.525200843811035\n",
      "test_accu: 0.6666666666666666  test loss: -6.444048464298248\n",
      "test_accu: 0.65  test loss: -6.171528071165085\n",
      "test_accu: 0.7333333333333333  test loss: -7.1274296045303345\n",
      "test_accu: 0.75  test loss: -7.163809150457382\n",
      "test_accu: 0.6833333333333333  test loss: -6.587533891201019\n",
      "test_accu: 0.7  test loss: -6.668786972761154\n",
      "test_accu: 0.6833333333333333  test loss: -6.661348938941956\n",
      "test_accu: 0.6  test loss: -5.8002621084451675\n",
      "test_accu: 0.6333333333333333  test loss: -6.359651982784271\n",
      "test_accu: 0.7333333333333333  test loss: -7.226448118686676\n",
      "test_accu: 0.5833333333333334  test loss: -5.645872116088867\n",
      "test_accu: 0.7666666666666667  test loss: -7.318978428840637\n",
      "test_accu: 0.6666666666666666  test loss: -6.4140191078186035\n",
      "test_accu: 0.6833333333333333  test loss: -6.70210599899292\n",
      "test_accu: 0.7  test loss: -6.829228013753891\n",
      "test_accu: 0.7666666666666667  test loss: -7.421710431575775\n",
      "test_accu: 0.7666666666666667  test loss: -7.526297360658646\n",
      "test_accu: 0.5833333333333334  test loss: -6.002159625291824\n",
      "test_accu: 0.7833333333333333  test loss: -7.765502512454987\n",
      "test_accu: 0.7  test loss: -6.66833233833313\n",
      "test_accu: 0.85  test loss: -8.26982969045639\n",
      "test_accu: 0.7  test loss: -6.856640577316284\n",
      "test_accu: 0.8166666666666667  test loss: -7.786524772644043\n",
      "test_accu: 0.7333333333333333  test loss: -7.109032690525055\n",
      "test_accu: 0.8166666666666667  test loss: -7.892455160617828\n",
      "test_accu: 0.7  test loss: -7.161747545003891\n",
      "test_accu: 0.7666666666666667  test loss: -7.411656588315964\n",
      "test_accu: 0.7833333333333333  test loss: -7.4526709616184235\n",
      "test_accu: 0.7166666666666667  test loss: -6.788913697004318\n",
      "test_accu: 0.6666666666666666  test loss: -6.515231549739838\n",
      "test_accu: 0.7666666666666667  test loss: -7.372799426317215\n",
      "test_accu: 0.6833333333333333  test loss: -6.790634959936142\n",
      "test_accu: 0.6666666666666666  test loss: -6.617992997169495\n",
      "test_accu: 0.7833333333333333  test loss: -7.485363692045212\n",
      "test_accu: 0.7  test loss: -6.845855712890625\n",
      "test_accu: 0.7  test loss: -6.841806381940842\n",
      "test_accu: 0.7333333333333333  test loss: -7.042115330696106\n",
      "test_accu: 0.7666666666666667  test loss: -7.420749545097351\n",
      "test_accu: 0.6833333333333333  test loss: -6.829236447811127\n",
      "test_accu: 0.6666666666666666  test loss: -6.402727946639061\n",
      "test_accu: 0.7666666666666667  test loss: -7.548767983913422\n",
      "test_accu: 0.7666666666666667  test loss: -7.444101691246033\n",
      "test_accu: 0.8166666666666667  test loss: -7.953740775585175\n",
      "0.12920902176390464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|██████████████████████████████████▍        | 8/10 [22:16<05:38, 169.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Avg Training Stats after 8 global rounds:\n",
      "Training Loss : -0.6268377693322011\n",
      "Train Accuracy: 83.33% \n",
      "\n",
      "\n",
      " | Global Training Round : 9 |\n",
      "\n",
      "[ 3.22095387 24.96948792  8.03634591 39.81422124  2.11577053  4.99825133\n",
      "  4.60549428 11.87564347 26.39477879 14.62608997]\n",
      "test_accu: 0.6666666666666666  test loss: -6.291552633047104\n",
      "test_accu: 0.7833333333333333  test loss: -7.624873101711273\n",
      "test_accu: 0.8  test loss: -7.832268476486206\n",
      "test_accu: 0.6166666666666667  test loss: -6.153100490570068\n",
      "test_accu: 0.7666666666666667  test loss: -7.364126294851303\n",
      "test_accu: 0.7833333333333333  test loss: -7.7523306012153625\n",
      "test_accu: 0.7  test loss: -6.534834116697311\n",
      "test_accu: 0.7  test loss: -7.035462379455566\n",
      "test_accu: 0.85  test loss: -8.285895764827728\n",
      "test_accu: 0.8  test loss: -7.717124313116074\n",
      "test_accu: 0.7166666666666667  test loss: -7.004228442907333\n",
      "test_accu: 0.7166666666666667  test loss: -6.966017186641693\n",
      "test_accu: 0.7166666666666667  test loss: -6.992292046546936\n",
      "test_accu: 0.6833333333333333  test loss: -6.724228203296661\n",
      "test_accu: 0.7166666666666667  test loss: -6.927155822515488\n",
      "test_accu: 0.7166666666666667  test loss: -6.880937993526459\n",
      "test_accu: 0.6  test loss: -5.800389409065247\n",
      "test_accu: 0.8  test loss: -7.717284381389618\n",
      "test_accu: 0.65  test loss: -6.439460098743439\n",
      "test_accu: 0.7666666666666667  test loss: -7.284930735826492\n",
      "test_accu: 0.7666666666666667  test loss: -7.527708888053894\n",
      "test_accu: 0.8166666666666667  test loss: -7.850043565034866\n",
      "test_accu: 0.6666666666666666  test loss: -6.42236453294754\n",
      "test_accu: 0.5333333333333333  test loss: -5.352147564291954\n",
      "test_accu: 0.65  test loss: -6.397976756095886\n",
      "test_accu: 0.6166666666666667  test loss: -5.972478479146957\n",
      "test_accu: 0.6333333333333333  test loss: -6.181086480617523\n",
      "test_accu: 0.7666666666666667  test loss: -7.448883056640625\n",
      "test_accu: 0.6833333333333333  test loss: -6.721598118543625\n",
      "test_accu: 0.8166666666666667  test loss: -7.998439848423004\n",
      "test_accu: 0.7166666666666667  test loss: -6.9350354969501495\n",
      "test_accu: 0.8166666666666667  test loss: -7.668990850448608\n",
      "test_accu: 0.7666666666666667  test loss: -7.444668412208557\n",
      "test_accu: 0.7  test loss: -7.00650942325592\n",
      "test_accu: 0.7333333333333333  test loss: -7.276380926370621\n",
      "test_accu: 0.6666666666666666  test loss: -6.305925101041794\n",
      "test_accu: 0.6833333333333333  test loss: -6.594545066356659\n",
      "test_accu: 0.7666666666666667  test loss: -7.439358979463577\n",
      "test_accu: 0.7333333333333333  test loss: -6.96038231253624\n",
      "test_accu: 0.7833333333333333  test loss: -7.719524085521698\n",
      "test_accu: 0.7  test loss: -6.879634648561478\n",
      "test_accu: 0.6666666666666666  test loss: -6.71625617146492\n",
      "test_accu: 0.7166666666666667  test loss: -7.01556146889925\n",
      "test_accu: 0.7  test loss: -6.819871574640274\n",
      "test_accu: 0.6  test loss: -6.034629940986633\n",
      "test_accu: 0.85  test loss: -8.272208631038666\n",
      "test_accu: 0.6666666666666666  test loss: -6.471772253513336\n",
      "test_accu: 0.6833333333333333  test loss: -6.861372768878937\n",
      "test_accu: 0.7333333333333333  test loss: -7.0035925805568695\n",
      "test_accu: 0.6833333333333333  test loss: -6.625166699290276\n",
      "test_accu: 0.6333333333333333  test loss: -6.259042903780937\n",
      "test_accu: 0.6666666666666666  test loss: -6.72129687666893\n",
      "test_accu: 0.7166666666666667  test loss: -6.9810755252838135\n",
      "test_accu: 0.8333333333333334  test loss: -7.993986964225769\n",
      "test_accu: 0.7  test loss: -6.870859533548355\n",
      "test_accu: 0.65  test loss: -6.4392848908901215\n",
      "test_accu: 0.6833333333333333  test loss: -6.67716172337532\n",
      "test_accu: 0.6666666666666666  test loss: -6.513719290494919\n",
      "test_accu: 0.6666666666666666  test loss: -6.314767777919769\n",
      "test_accu: 0.7333333333333333  test loss: -7.1770550310611725\n",
      "test_accu: 0.7666666666666667  test loss: -7.225930750370026\n",
      "test_accu: 0.7166666666666667  test loss: -6.809692919254303\n",
      "test_accu: 0.7166666666666667  test loss: -6.852971017360687\n",
      "test_accu: 0.7  test loss: -6.730707377195358\n",
      "test_accu: 0.6166666666666667  test loss: -5.9158540070056915\n",
      "test_accu: 0.65  test loss: -6.4058613777160645\n",
      "test_accu: 0.7666666666666667  test loss: -7.445526212453842\n",
      "test_accu: 0.5666666666666667  test loss: -5.63334983587265\n",
      "test_accu: 0.7166666666666667  test loss: -7.272622108459473\n",
      "test_accu: 0.6666666666666666  test loss: -6.44223690032959\n",
      "test_accu: 0.6833333333333333  test loss: -6.688912481069565\n",
      "test_accu: 0.7  test loss: -6.8515501618385315\n",
      "test_accu: 0.7666666666666667  test loss: -7.506860613822937\n",
      "test_accu: 0.7666666666666667  test loss: -7.56369948387146\n",
      "test_accu: 0.6  test loss: -6.045425862073898\n",
      "test_accu: 0.7833333333333333  test loss: -7.782825291156769\n",
      "test_accu: 0.7  test loss: -6.727210104465485\n",
      "test_accu: 0.8666666666666667  test loss: -8.283534824848175\n",
      "test_accu: 0.6833333333333333  test loss: -6.837994694709778\n",
      "test_accu: 0.8166666666666667  test loss: -7.8638763427734375\n",
      "test_accu: 0.7333333333333333  test loss: -7.175078243017197\n",
      "test_accu: 0.8  test loss: -7.9117445051670074\n",
      "test_accu: 0.7333333333333333  test loss: -7.13918724656105\n",
      "test_accu: 0.7666666666666667  test loss: -7.452864229679108\n",
      "test_accu: 0.7833333333333333  test loss: -7.471264272928238\n",
      "test_accu: 0.7  test loss: -6.786787271499634\n",
      "test_accu: 0.65  test loss: -6.479290336370468\n",
      "test_accu: 0.7666666666666667  test loss: -7.450578421354294\n",
      "test_accu: 0.6833333333333333  test loss: -6.68537700176239\n",
      "test_accu: 0.65  test loss: -6.4517165422439575\n",
      "test_accu: 0.8  test loss: -7.550489157438278\n",
      "test_accu: 0.7166666666666667  test loss: -6.881772547960281\n",
      "test_accu: 0.7  test loss: -6.9278401136398315\n",
      "test_accu: 0.7333333333333333  test loss: -7.157316446304321\n",
      "test_accu: 0.7833333333333333  test loss: -7.4357969760894775\n",
      "test_accu: 0.6833333333333333  test loss: -6.836221098899841\n",
      "test_accu: 0.6666666666666666  test loss: -6.426182776689529\n",
      "test_accu: 0.7666666666666667  test loss: -7.625348210334778\n",
      "test_accu: 0.7833333333333333  test loss: -7.533676266670227\n",
      "test_accu: 0.8  test loss: -7.887853562831879\n",
      "0.05929034969621342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|██████████████████████████████████████▋    | 9/10 [25:13<02:51, 171.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " | Global Training Round : 10 |\n",
      "\n",
      "[19.08790479 11.59851118 23.93360241  1.11618352 17.56014152  1.8176508\n",
      " 39.01032602 20.94341624  4.26767521  5.59557786]\n",
      "test_accu: 0.6666666666666666  test loss: -6.337122738361359\n",
      "test_accu: 0.7666666666666667  test loss: -7.658271968364716\n",
      "test_accu: 0.7833333333333333  test loss: -7.779139131307602\n",
      "test_accu: 0.6333333333333333  test loss: -6.1581562757492065\n",
      "test_accu: 0.7666666666666667  test loss: -7.512011557817459\n",
      "test_accu: 0.7833333333333333  test loss: -7.770710587501526\n",
      "test_accu: 0.6666666666666666  test loss: -6.4367005825042725\n",
      "test_accu: 0.7166666666666667  test loss: -7.126428127288818\n",
      "test_accu: 0.8666666666666667  test loss: -8.27217048406601\n",
      "test_accu: 0.8  test loss: -7.704126268625259\n",
      "test_accu: 0.7166666666666667  test loss: -6.955875098705292\n",
      "test_accu: 0.6833333333333333  test loss: -6.754642486572266\n",
      "test_accu: 0.7166666666666667  test loss: -7.173087894916534\n",
      "test_accu: 0.7  test loss: -6.789629548788071\n",
      "test_accu: 0.7166666666666667  test loss: -6.817343533039093\n",
      "test_accu: 0.7  test loss: -6.864388197660446\n",
      "test_accu: 0.6  test loss: -5.821782484650612\n",
      "test_accu: 0.8333333333333334  test loss: -7.747471511363983\n",
      "test_accu: 0.6666666666666666  test loss: -6.5529367327690125\n",
      "test_accu: 0.75  test loss: -7.250546514987946\n",
      "test_accu: 0.7833333333333333  test loss: -7.563479840755463\n",
      "test_accu: 0.8166666666666667  test loss: -7.830913215875626\n",
      "test_accu: 0.6666666666666666  test loss: -6.454925119876862\n",
      "test_accu: 0.55  test loss: -5.319708796218038\n",
      "test_accu: 0.65  test loss: -6.395634829998016\n",
      "test_accu: 0.6166666666666667  test loss: -6.072169691324234\n",
      "test_accu: 0.65  test loss: -6.257397621870041\n",
      "test_accu: 0.7666666666666667  test loss: -7.427158832550049\n",
      "test_accu: 0.6666666666666666  test loss: -6.613040626049042\n",
      "test_accu: 0.8333333333333334  test loss: -8.016486883163452\n",
      "test_accu: 0.7166666666666667  test loss: -6.839146673679352\n",
      "test_accu: 0.7666666666666667  test loss: -7.559047907590866\n",
      "test_accu: 0.7666666666666667  test loss: -7.4747374057769775\n",
      "test_accu: 0.7166666666666667  test loss: -7.0568927526474\n",
      "test_accu: 0.7333333333333333  test loss: -7.278648793697357\n",
      "test_accu: 0.65  test loss: -6.364806681871414\n",
      "test_accu: 0.6833333333333333  test loss: -6.693465709686279\n",
      "test_accu: 0.75  test loss: -7.492660224437714\n",
      "test_accu: 0.7166666666666667  test loss: -6.966273844242096\n",
      "test_accu: 0.7666666666666667  test loss: -7.756814181804657\n",
      "test_accu: 0.7  test loss: -6.908123075962067\n",
      "test_accu: 0.7333333333333333  test loss: -6.926965951919556\n",
      "test_accu: 0.7166666666666667  test loss: -7.097062736749649\n",
      "test_accu: 0.6833333333333333  test loss: -6.762689650058746\n",
      "test_accu: 0.6  test loss: -6.035428315401077\n",
      "test_accu: 0.85  test loss: -8.272447764873505\n",
      "test_accu: 0.6666666666666666  test loss: -6.471486985683441\n",
      "test_accu: 0.6833333333333333  test loss: -6.991932511329651\n",
      "test_accu: 0.7666666666666667  test loss: -7.194393455982208\n",
      "test_accu: 0.7  test loss: -6.68543803691864\n",
      "test_accu: 0.65  test loss: -6.327648237347603\n",
      "test_accu: 0.6833333333333333  test loss: -6.768886178731918\n",
      "test_accu: 0.7166666666666667  test loss: -7.02782467007637\n",
      "test_accu: 0.85  test loss: -8.15389883518219\n",
      "test_accu: 0.7166666666666667  test loss: -7.01676407456398\n",
      "test_accu: 0.6333333333333333  test loss: -6.439904987812042\n",
      "test_accu: 0.7  test loss: -6.669940531253815\n",
      "test_accu: 0.6666666666666666  test loss: -6.48568531870842\n",
      "test_accu: 0.6666666666666666  test loss: -6.524062126874924\n",
      "test_accu: 0.7166666666666667  test loss: -7.0991014540195465\n",
      "test_accu: 0.75  test loss: -7.172180712223053\n",
      "test_accu: 0.7  test loss: -6.737445414066315\n",
      "test_accu: 0.6833333333333333  test loss: -6.573565512895584\n",
      "test_accu: 0.6833333333333333  test loss: -6.654877841472626\n",
      "test_accu: 0.6166666666666667  test loss: -5.919412434101105\n",
      "test_accu: 0.65  test loss: -6.358443260192871\n",
      "test_accu: 0.7833333333333333  test loss: -7.507493287324905\n",
      "test_accu: 0.5666666666666667  test loss: -5.70634263753891\n",
      "test_accu: 0.7666666666666667  test loss: -7.428596079349518\n",
      "test_accu: 0.6666666666666666  test loss: -6.477488666772842\n",
      "test_accu: 0.6666666666666666  test loss: -6.699645668268204\n",
      "test_accu: 0.6833333333333333  test loss: -6.809136003255844\n",
      "test_accu: 0.7833333333333333  test loss: -7.612368643283844\n",
      "test_accu: 0.7666666666666667  test loss: -7.553902566432953\n",
      "test_accu: 0.5833333333333334  test loss: -6.012199223041534\n",
      "test_accu: 0.8  test loss: -7.825648128986359\n",
      "test_accu: 0.7166666666666667  test loss: -6.812839522957802\n",
      "test_accu: 0.8333333333333334  test loss: -8.275901198387146\n",
      "test_accu: 0.6833333333333333  test loss: -6.827260136604309\n",
      "test_accu: 0.8  test loss: -7.859403729438782\n",
      "test_accu: 0.75  test loss: -7.1594028770923615\n",
      "test_accu: 0.8  test loss: -7.936361908912659\n",
      "test_accu: 0.7166666666666667  test loss: -7.215609401464462\n",
      "test_accu: 0.7666666666666667  test loss: -7.437620788812637\n",
      "test_accu: 0.7833333333333333  test loss: -7.431412428617477\n",
      "test_accu: 0.7166666666666667  test loss: -6.914069384336472\n",
      "test_accu: 0.6666666666666666  test loss: -6.503712207078934\n",
      "test_accu: 0.75  test loss: -7.430250376462936\n",
      "test_accu: 0.6833333333333333  test loss: -6.729560136795044\n",
      "test_accu: 0.6333333333333333  test loss: -6.491101235151291\n",
      "test_accu: 0.8  test loss: -7.594292193651199\n",
      "test_accu: 0.7166666666666667  test loss: -6.814114898443222\n",
      "test_accu: 0.7  test loss: -6.935531497001648\n",
      "test_accu: 0.7333333333333333  test loss: -7.192939341068268\n",
      "test_accu: 0.7666666666666667  test loss: -7.461414068937302\n",
      "test_accu: 0.6833333333333333  test loss: -6.642704129219055\n",
      "test_accu: 0.6666666666666666  test loss: -6.387583434581757\n",
      "test_accu: 0.7666666666666667  test loss: -7.591187477111816\n",
      "test_accu: 0.7833333333333333  test loss: -7.6227743327617645\n",
      "test_accu: 0.8  test loss: -7.875238299369812\n",
      "0.10246209361932965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 10/10 [28:04<00:00, 168.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Avg Training Stats after 10 global rounds:\n",
      "Training Loss : -0.6451082590003303\n",
      "Train Accuracy: 81.67% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Set the model to train and send it to device.\n",
    "global_model.to(device)\n",
    "global_model.train()\n",
    "print(global_model)\n",
    "\n",
    "# copy weights\n",
    "global_weights = global_model.state_dict()\n",
    "\n",
    "# Training\n",
    "train_loss, train_accuracy = [], []\n",
    "val_acc_list, net_list = [], []\n",
    "cv_loss, cv_acc = [], []\n",
    "print_every = 2\n",
    "val_loss_pre, counter = 0, 0\n",
    "test_accs_epochs=[]\n",
    "noise_levels_epochs=[]\n",
    "res_times_epochs=[]\n",
    "selected_user_idxs_epochs=[]\n",
    "average_time_epochs=[]\n",
    "max_time_epochs=[]\n",
    "seed=22\n",
    "# np.random.seed(seed)\n",
    "\n",
    "for epoch in tqdm(range(args.epochs)):\n",
    "    test_accs=[]\n",
    "    local_weights, local_losses = [], []\n",
    "    print(f'\\n | Global Training Round : {epoch+1} |\\n')\n",
    "\n",
    "    global_model.train()\n",
    "    # m = max(int(args.frac * args.num_users), 1)\n",
    "    # idxs_users = np.random.choice(range(args.num_users), m, replace=False)\n",
    "    idxs_users = np.arange(args.num_users)\n",
    "    res_times=np.random.exponential(scale=10, size=args.num_users)\n",
    "    print(res_times[:10])\n",
    "    res_times_epochs.append(res_times)\n",
    "    sorted_time_idx=np.argsort(res_times, axis=- 1, kind=None, order=None)\n",
    "    noise_levels = np.random.uniform(low=0, high=1, size=args.num_users) # noise level for norm dist, 0 mean, this is var\n",
    "    noise_levels_epochs.append(noise_levels)\n",
    "    for idx in idxs_users:\n",
    "        local_model = LocalUpdate(args=args, dataset=train_dataset,\n",
    "                                  idxs=user_groups[idx], logger=logger, noise_level=noise_levels[idx], seed=seed)\n",
    "        w, loss, test_acc = local_model.update_weights(\n",
    "            model=copy.deepcopy(global_model), global_round=epoch)\n",
    "        local_weights.append(copy.deepcopy(w))\n",
    "        local_losses.append(copy.deepcopy(loss))\n",
    "        test_accs.append(test_acc)\n",
    "    test_accs_epochs.append(test_accs)\n",
    "\n",
    "    # first stage, get m and Cm\n",
    "    Cm = 0\n",
    "    m = 0\n",
    "    for i in range(a_star):\n",
    "        if (Cm < test_accs[sorted_time_idx[i]]):        \n",
    "            Cm = test_accs[sorted_time_idx[i]]\n",
    "            print(res_times[sorted_time_idx[i]])\n",
    "            m=i\n",
    "    \n",
    "    # second stage:\n",
    "    selected_user_idxs=[]\n",
    "    selected_local_weights=[]\n",
    "    # print(res_times_epochs[0][selected_user_idxs_epochs[0][0]])\n",
    "    average_time=0\n",
    "    max_time=0\n",
    "    for i in range(a_star, args.num_users):\n",
    "        if len(selected_user_idxs) + (args.num_users-i) <= R:\n",
    "            selected_user_idxs.append(sorted_time_idx[i])\n",
    "            selected_local_weights.append(copy.deepcopy(local_weights[sorted_time_idx[i]]))\n",
    "            average_time+=res_times[sorted_time_idx[i]]\n",
    "            max_time=max(max_time, res_times[sorted_time_idx[i]])\n",
    "\n",
    "        \n",
    "        elif test_accs[sorted_time_idx[i]] > Cm:\n",
    "            selected_user_idxs.append(sorted_time_idx[i])\n",
    "            selected_local_weights.append(copy.deepcopy(local_weights[sorted_time_idx[i]]))\n",
    "            average_time+=res_times[sorted_time_idx[i]]\n",
    "            max_time=max(max_time, res_times[sorted_time_idx[i]])\n",
    "\n",
    "        if len(selected_user_idxs)==R:\n",
    "            break;\n",
    "    average_time = average_time/len(selected_user_idxs)\n",
    "    average_time_epochs.append(average_time)\n",
    "    max_time_epochs.append(max_time)\n",
    "    # print(len(local_weights))\n",
    "    # print(len(local_weights[0]))\n",
    "    # print(len(selected_local_weights))\n",
    "    # print(len(selected_local_weights[0]))\n",
    "    selected_user_idxs_epochs.append(selected_user_idxs)\n",
    "    # update global weights\n",
    "    # global_weights = average_weights(local_weights)\n",
    "    global_weights = average_weights(selected_local_weights)\n",
    "\n",
    "    # update global weights\n",
    "    global_model.load_state_dict(global_weights)\n",
    "\n",
    "    loss_avg = sum(local_losses) / len(local_losses)\n",
    "    train_loss.append(loss_avg)\n",
    "\n",
    "    # Calculate avg training accuracy over all users at every epoch\n",
    "    list_acc, list_loss = [], []\n",
    "    global_model.eval()\n",
    "    for c in range(args.num_users):\n",
    "        local_model = LocalUpdate(args=args, dataset=train_dataset,\n",
    "                                  idxs=user_groups[idx], logger=logger, noise_level=noise_levels[idx], seed=seed)\n",
    "        acc, loss = local_model.inference(model=global_model)\n",
    "        list_acc.append(acc)\n",
    "        list_loss.append(loss)\n",
    "    train_accuracy.append(sum(list_acc)/len(list_acc))\n",
    "\n",
    "    # print global training loss after every 'i' rounds\n",
    "    if (epoch+1) % print_every == 0:\n",
    "        print(f' \\nAvg Training Stats after {epoch+1} global rounds:')\n",
    "        print(f'Training Loss : {np.mean(np.array(train_loss))}')\n",
    "        print('Train Accuracy: {:.2f}% \\n'.format(100*train_accuracy[-1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20762342-1a2b-499e-a6c6-051cde869b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " Results after 10 global rounds of training:\n",
      "|---- Avg Train Accuracy: 81.67%\n",
      "|---- Test Accuracy: 72.60%\n",
      "\n",
      " Total Run Time: 1685.9301\n",
      "7.107017588058413\n",
      "17.39002727224438\n"
     ]
    }
   ],
   "source": [
    "# Test inference after completion of training\n",
    "test_acc, test_loss = test_inference(args, global_model, test_dataset)\n",
    "\n",
    "print(f' \\n Results after {args.epochs} global rounds of training:')\n",
    "print(\"|---- Avg Train Accuracy: {:.2f}%\".format(100*train_accuracy[-1]))\n",
    "print(\"|---- Test Accuracy: {:.2f}%\".format(100*test_acc))\n",
    "\n",
    "# Saving the objects train_loss and train_accuracy:\n",
    "file_name = '../save/objects/{}_{}_{}_C[{}]_iid[{}]_E[{}]_B[{}].pkl'.\\\n",
    "    format(args.dataset, args.model, args.epochs, args.frac, args.iid,\n",
    "           args.local_ep, args.local_bs)\n",
    "\n",
    "with open(file_name, 'wb') as f:\n",
    "    pickle.dump([train_loss, train_accuracy], f)\n",
    "\n",
    "print('\\n Total Run Time: {0:0.4f}'.format(time.time()-start_time))\n",
    "\n",
    "print(np.mean(average_time_epochs))\n",
    "print(np.mean(max_time_epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7894e629-2be7-492c-86fc-7b6555de6ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4141865024820075, 0.6517140076209227, 1.3902732811015903, 0.6789471953567091, 14.783232558384446, 1.2442909356855052, 1.1582732699687817, 24.97593299590069, 0.7306862604933635, 25.04263887359011]\n"
     ]
    }
   ],
   "source": [
    "print(average_time_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29da1bf4-5d58-4105-9e45-7c69ff1243cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(res_times_epochs[0][selected_user_idxs_epochs[0][0]])\n",
    "# print(res_times_epochs[0][selected_user_idxs_epochs[0][1]])\n",
    "# print(res_times_epochs[0][selected_user_idxs_epochs[0][2]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cefad019-8e46-4098-9291-d7d3541afecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOTTING (optional)\n",
    "# import matplotlib\n",
    "# import matplotlib.pyplot as plt\n",
    "# matplotlib.use('Agg')\n",
    "\n",
    "# Plot Loss curve\n",
    "# plt.figure()\n",
    "# plt.title('Training Loss vs Communication rounds')\n",
    "# plt.plot(range(len(train_loss)), train_loss, color='r')\n",
    "# plt.ylabel('Training loss')\n",
    "# plt.xlabel('Communication Rounds')\n",
    "# plt.savefig('../save/fed_{}_{}_{}_C[{}]_iid[{}]_E[{}]_B[{}]_loss.png'.\n",
    "#             format(args.dataset, args.model, args.epochs, args.frac,\n",
    "#                    args.iid, args.local_ep, args.local_bs))\n",
    "#\n",
    "# # Plot Average Accuracy vs Communication rounds\n",
    "# plt.figure()\n",
    "# plt.title('Average Accuracy vs Communication rounds')\n",
    "# plt.plot(range(len(train_accuracy)), train_accuracy, color='k')\n",
    "# plt.ylabel('Average Accuracy')\n",
    "# plt.xlabel('Communication Rounds')\n",
    "# plt.savefig('../save/fed_{}_{}_{}_C[{}]_iid[{}]_E[{}]_B[{}]_acc.png'.\n",
    "#             format(args.dataset, args.model, args.epochs, args.frac,\n",
    "#                    args.iid, args.local_ep, args.local_bs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fe6dee27-c8f4-492c-b306-d5b9fef9f0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(test_accs[selected_user_idxs])\n",
    "# plt.plot(test_accs[selected_user_idxs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e16dbe3-b34d-4ec6-81f6-6fccb4113332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0. 24. 16. 10.  7.  5.  3.  2.  1.  1.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "max_as=np.zeros(100)\n",
    "for R in range(1,100):\n",
    "    for r1 in range(1,R):\n",
    "        for r2 in range(R,100):\n",
    "            a_star=int(args.num_users*np.exp( -1*np.power( np.math.factorial(r2)/np.math.factorial(r1-1) , 1/(r2-r1+1) ) ))\n",
    "            max_as[R] = max(max_as[R],a_star)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6eb8e0aa-52d9-4ce0-a5a8-583044451400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(max_as)):\n",
    "#     print(max_as[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3558dc26-1f43-4b7d-a46b-3931311f4868",
   "metadata": {},
   "outputs": [],
   "source": [
    "r1=1\n",
    "r2=10\n",
    "R=10\n",
    "a_star=args.num_users*np.exp( -1*np.power( np.math.factorial(r2)/np.math.factorial(r1-1) , 1/(r2-r1+1) ) )\n",
    "a_star=int(a_star)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
